{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DIAG] Category coverage:\n",
      "  - Medulla: 58 nodes; depth range 4–7 (prefix /997/8/343/1065/354/)\n",
      "  - Pons: 34 nodes; depth range 4–7 (prefix /997/8/343/1065/771/)\n",
      "  - Hypothalamus: 57 nodes; depth range 4–8 (prefix /997/8/343/1129/1097/)\n",
      "  - Thalamus: 67 nodes; depth range 4–8 (prefix /997/8/343/1129/549/)\n",
      "  - Midbrain: 68 nodes; depth range 3–7 (prefix /997/8/343/313/)\n",
      "  - Cerebellum: 28 nodes; depth range 2–6 (prefix /997/8/512/)\n",
      "  - Cortical plate: 352 nodes; depth range 4–9 (prefix /997/8/567/688/695/)\n",
      "  - Cortical subplate: 14 nodes; depth range 4–6 (prefix /997/8/567/688/703/)\n",
      "  - Pallidum: 15 nodes; depth range 4–7 (prefix /997/8/567/623/803/)\n",
      "  - Striatum: 23 nodes; depth range 4–7 (prefix /997/8/567/623/477/)\n",
      "\n",
      "[SELECT] 160 nodes, depth 3–7\n",
      "[SELECT] Selected 160 collapsed nodes within categories. Depth range: 3–7\n",
      "[OK] Collapsed matrices written → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsed_matrix.xlsx\n",
      "[OK] Contribution log written → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsing_log.csv (rows=17878)\n",
      "[AUDIT] collapsed_selection → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\collapsed_selection.csv (160 rows)\n",
      "[AUDIT] child_to_collapsed_map → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\child_to_collapsed_map.csv (687 rows)\n",
      "[AUDIT] unmapped_regions → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\unmapped_regions.csv  (count=153)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "structures_path = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\structures.csv\"\n",
    "input_excel     = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\per_mouse_sheets.xlsx\"\n",
    "output_excel    = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsed_matrix.xlsx\"\n",
    "output_log      = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsing_log.csv\"\n",
    "audit_dir       = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\"\n",
    "\n",
    "TARGET_N        = 160\n",
    "ROOT_ID         = \"997\"   # Allen mouse root\n",
    "BAN_IDS         = {\"8\"}   # ban shallow hub: \"Basic cell groups and regions\"\n",
    "\n",
    "# Category roots: nothing ABOVE these IDs will ever be selected\n",
    "category_map = {\n",
    "    \"Medulla\": \"/997/8/343/1065/354/\",\n",
    "    \"Pons\": \"/997/8/343/1065/771/\",\n",
    "    \"Hypothalamus\": \"/997/8/343/1129/1097/\",\n",
    "    \"Thalamus\": \"/997/8/343/1129/549/\",\n",
    "    \"Midbrain\": \"/997/8/343/313/\",\n",
    "    \"Cerebellum\": \"/997/8/512/\",\n",
    "    \"Cortical plate\": \"/997/8/567/688/695/\",\n",
    "    \"Cortical subplate\": \"/997/8/567/688/703/\",\n",
    "    \"Pallidum\": \"/997/8/567/623/803/\",\n",
    "    \"Striatum\": \"/997/8/567/623/477/\",\n",
    "}\n",
    "# Selection behavior inside categories\n",
    "MIN_OFFSET_BELOW_CATEGORY = 1     # require at least 1 level below the category root\n",
    "ALLOW_CATEGORY_ROOTS      = False # forbid selecting the category root itself initially\n",
    "RELAX_DEPTH_IF_N_SHORT    = True  # carefully relax toward the root if needed\n",
    "# ==================================================\n",
    "def canon(p) -> str:\n",
    "    \"\"\"Ensure leading and trailing slash: '/.../'.\"\"\"\n",
    "    p = \"\" if p is None else str(p)\n",
    "    return \"/\" + p.strip(\"/\") + \"/\"\n",
    "\n",
    "\n",
    "# ------------------ Structures & Graph ------------------\n",
    "def load_structures(structures_csv: str):\n",
    "    S = pd.read_csv(structures_path)\n",
    "    S[\"id\"] = S[\"id\"].astype(str)\n",
    "    S[\"structure_id_path\"] = S[\"structure_id_path\"].apply(canon)\n",
    "\n",
    "    id_to_acronym = S.set_index(\"id\")[\"acronym\"].astype(str).to_dict()\n",
    "    id_to_path    = S.set_index(\"id\")[\"structure_id_path\"].to_dict()  # canonical now\n",
    "    id_to_name    = S.set_index(\"id\")[\"name\"].astype(str).to_dict()\n",
    "    return S, id_to_name, id_to_path, id_to_acronym\n",
    "\n",
    "def build_graph_from_paths(S: pd.DataFrame) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in S.iterrows():\n",
    "        # row[\"structure_id_path\"] is already canonical '/.../'\n",
    "        parts = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "        for i in range(len(parts) - 1):\n",
    "            G.add_edge(parts[i], parts[i + 1])  # IDs are strings\n",
    "    return G\n",
    "\n",
    "\n",
    "def node_depth_from_path(sid: str, id_to_path: dict) -> int:\n",
    "    p = id_to_path.get(sid, \"\")\n",
    "    return max(len(str(p).strip(\"/\").split(\"/\")) - 1, 0)\n",
    "\n",
    "# ------------------ Diagnostics ------------------\n",
    "def diagnose_categories(id_to_path: dict, category_map: dict):\n",
    "    print(\"\\n[DIAG] Category coverage:\")\n",
    "    for name, pref in category_map.items():\n",
    "        pref_s = canon(pref)                     # <-- canonicalize the prefix\n",
    "        depths, count = [], 0\n",
    "        for sid, pth in id_to_path.items():      # pth already canonical\n",
    "            if pth.startswith(pref_s):\n",
    "                count += 1\n",
    "                depths.append(len(pth.strip(\"/\").split(\"/\")) - 1)\n",
    "        if count == 0:\n",
    "            print(f\"  - {name}: 0 nodes under prefix {pref_s}  <<< CHECK THIS PREFIX\")\n",
    "        else:\n",
    "            print(f\"  - {name}: {count} nodes; depth range {min(depths)}–{max(depths)} (prefix {pref_s})\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# --------- Category-bounded antichain selection ----------\n",
    "def select_frontier_antichain_within_categories(\n",
    "    G,\n",
    "    id_to_path: dict,\n",
    "    category_map: dict,\n",
    "    target_n: int,\n",
    "    root_id: str = \"997\",\n",
    "    *,\n",
    "    min_offset: int = 1,     # 1 = start frontier at children of category roots\n",
    "    max_extra_depth: int = 3, # don’t go deeper than (root_depth + 3)\n",
    "    ban_ids: set | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Coverage-preserving frontier selector:\n",
    "    - Works inside the given category subtrees (never above them).\n",
    "    - Builds an antichain 'frontier' that covers all leaves: start from each\n",
    "      category root (or its children if min_offset=1), then iteratively replace\n",
    "      the frontier node with the largest subtree by ALL of its children, while:\n",
    "        * respecting a per-category max depth (root_depth + max_extra_depth),\n",
    "        * never selecting banned IDs,\n",
    "        * keeping an antichain (no ancestor/descendant pairs).\n",
    "\n",
    "    Returns a list[str] of selected region IDs (strings).\n",
    "    \"\"\"\n",
    "    ban_ids = set(map(str, ban_ids or set()))\n",
    "    root_id = str(root_id)\n",
    "\n",
    "    # --- helpers ---\n",
    "    def canon(p) -> str:\n",
    "        return \"/\" + str(p).strip(\"/\") + \"/\"\n",
    "\n",
    "    cat_prefixes = {k: canon(v) for k, v in category_map.items()}\n",
    "    cat_roots = {pref.strip(\"/\").split(\"/\")[-1] for pref in cat_prefixes.values()}\n",
    "\n",
    "    def depth(sid: str) -> int:\n",
    "        p = id_to_path.get(sid, \"\")\n",
    "        return max(len(str(p).strip(\"/\").split(\"/\")) - 1, 0)\n",
    "\n",
    "    # Map each node to its (longest) matching category root id, or None\n",
    "    node_catroot = {}\n",
    "    for sid, pth in id_to_path.items():\n",
    "        best = None; best_len = -1\n",
    "        for pref in cat_prefixes.values():\n",
    "            if pth.startswith(pref):\n",
    "                L = len(pref)\n",
    "                if L > best_len:\n",
    "                    best_len = L\n",
    "                    best = pref.strip(\"/\").split(\"/\")[-1]\n",
    "        node_catroot[sid] = best\n",
    "\n",
    "    # Per-category depth limit\n",
    "    cat_root_depth = {r: depth(r) for r in cat_roots}\n",
    "\n",
    "    def in_cat(sid: str):\n",
    "        cr = node_catroot.get(sid)\n",
    "        return (cr is not None), cr\n",
    "\n",
    "    def within_limit(sid: str, cr: str):\n",
    "        return depth(sid) <= cat_root_depth[cr] + max_extra_depth\n",
    "\n",
    "    def children_in_cat(sid: str, cr: str):\n",
    "        return [c for c in G.successors(sid) if node_catroot.get(c) == cr]\n",
    "\n",
    "    # Count descendant leaves within the same category (memoized)\n",
    "    from functools import lru_cache\n",
    "    @lru_cache(maxsize=None)\n",
    "    def leaf_count_in_cat(sid: str, cr: str) -> int:\n",
    "        kids = children_in_cat(sid, cr)\n",
    "        if not kids or not within_limit(sid, cr):\n",
    "            return 1  # treat as terminal for our frontier decision\n",
    "        return sum(leaf_count_in_cat(k, cr) for k in kids)\n",
    "\n",
    "    # --- initial frontier: for each category, roots (offset=0) or their children (offset=1) ---\n",
    "    frontier = []\n",
    "    for name, pref in cat_prefixes.items():\n",
    "        cr = pref.strip(\"/\").split(\"/\")[-1]\n",
    "        if cr in ban_ids:  # skip banned roots entirely\n",
    "            continue\n",
    "        if min_offset <= 0:\n",
    "            frontier.append(cr)\n",
    "        else:\n",
    "            kids = [c for c in G.successors(cr) if node_catroot.get(c) == cr]\n",
    "            if not kids:  # degenerate branch: keep the root if nothing else exists\n",
    "                frontier.append(cr)\n",
    "            else:\n",
    "                frontier.extend(kids)\n",
    "\n",
    "    # Keep only valid frontier nodes (inside categories, not banned, within depth limit)\n",
    "    clean = []\n",
    "    for v in frontier:\n",
    "        inside, cr = in_cat(v)\n",
    "        if not inside or v in ban_ids:\n",
    "            continue\n",
    "        if not within_limit(v, cr):\n",
    "            continue\n",
    "        clean.append(v)\n",
    "    frontier = list(dict.fromkeys(clean))  # unique, stable order\n",
    "\n",
    "    # Enforce antichain on the starting frontier\n",
    "    sel = []\n",
    "    for v in frontier:\n",
    "        if any((v in nx.descendants(G, s)) or (s in nx.descendants(G, v)) for s in sel):\n",
    "            continue\n",
    "        sel.append(v)\n",
    "\n",
    "    # --- refine frontier until ~target_n, by expanding largest subtree ---\n",
    "    # We only ever replace a selected node by ALL of its children inside the same category.\n",
    "    def expandable(v):\n",
    "        inside, cr = in_cat(v)\n",
    "        return inside and any(children_in_cat(v, cr)) and depth(v) < cat_root_depth[cr] + max_extra_depth\n",
    "\n",
    "    # Greedy expansion with fit-to-budget (don’t exceed target_n)\n",
    "    while len(sel) < target_n:\n",
    "        # pick expandable node with largest leaf_count\n",
    "        candidates = [(leaf_count_in_cat(v, in_cat(v)[1]), v) for v in sel if expandable(v)]\n",
    "        if not candidates:\n",
    "            break\n",
    "        candidates.sort(reverse=True)\n",
    "        expanded = False\n",
    "        for _, v in candidates:\n",
    "            cr = in_cat(v)[1]\n",
    "            kids = [k for k in children_in_cat(v, cr) if within_limit(k, cr) and k not in ban_ids]\n",
    "            if not kids:\n",
    "                continue\n",
    "            # Check budget if we replace v with all its kids\n",
    "            new_count = len(sel) - 1 + len(kids)\n",
    "            if new_count > target_n:\n",
    "                # try a smaller expandable node if available\n",
    "                continue\n",
    "            # do the replacement (preserves antichain and coverage)\n",
    "            sel.remove(v)\n",
    "            # ensure kids don't conflict with existing selection (they shouldn't, but be safe)\n",
    "            for k in kids:\n",
    "                if any((k in nx.descendants(G, s)) or (s in nx.descendants(G, k)) for s in sel):\n",
    "                    # if conflict, skip this whole expansion to preserve antichain\n",
    "                    sel.insert(0, v)  # put v back (order not important)\n",
    "                    break\n",
    "            else:\n",
    "                sel.extend(kids)\n",
    "                expanded = True\n",
    "                break\n",
    "        if not expanded:\n",
    "            break  # no expansion fits in budget\n",
    "\n",
    "    # Final antichain check\n",
    "    S = set(sel)\n",
    "    for u in list(S):\n",
    "        if S & set(nx.descendants(G, u)):\n",
    "            raise AssertionError(f\"Antichain violation: {u} has selected descendant(s).\")\n",
    "\n",
    "    return sel\n",
    "\n",
    "# -------------- Mapping & Aggregation --------------\n",
    "def map_child_to_collapsed(id_to_path: dict, selected_ids: list[str]) -> dict:\n",
    "    \"\"\"Map every atlas region id -> nearest selected ancestor (if any).\"\"\"\n",
    "    selected = set(selected_ids)\n",
    "    mapping = {}\n",
    "    for sid, pth in id_to_path.items():\n",
    "        parts = [p for p in str(pth).strip(\"/\").split(\"/\") if p]\n",
    "        mapped = next((p for p in reversed(parts) if p in selected), None)\n",
    "        if mapped:\n",
    "            mapping[sid] = mapped\n",
    "    return mapping\n",
    "\n",
    "def aggregate_per_mouse_hemi(input_excel: str,\n",
    "                             structure_to_collapsed: dict,\n",
    "                             id_to_name: dict,\n",
    "                             id_to_path: dict):\n",
    "    \"\"\"\n",
    "    Read per_mouse_sheets.xlsx and aggregate mean/std/sem across child regions, per mouse & hemisphere.\n",
    "    Accepts L/R or any *_L / *_R columns.\n",
    "    Reconstructs structure_id_path from region_id if needed.\n",
    "    \"\"\"\n",
    "    df_by_mouse = pd.read_excel(input_excel, sheet_name=None)\n",
    "    mean_L, std_L, sem_L = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "    mean_R, std_R, sem_R = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "    log_rows = []\n",
    "\n",
    "    for sheet_name, df_mouse in df_by_mouse.items():\n",
    "        if str(sheet_name).lower() == \"summary\":\n",
    "            continue\n",
    "\n",
    "        # Ensure structure_id_path exists (reconstruct from region_id if available)\n",
    "        if \"structure_id_path\" not in df_mouse.columns:\n",
    "            if \"region_id\" in df_mouse.columns:\n",
    "                df_mouse = df_mouse.copy()\n",
    "                df_mouse[\"structure_id_path\"] = df_mouse[\"region_id\"].astype(str).map(id_to_path).fillna(\"\")\n",
    "            else:\n",
    "                print(f\"[WARN] {sheet_name}: missing structure_id_path; skipping.\")\n",
    "                continue\n",
    "\n",
    "        # Detect hemisphere columns (L/R). Fallback: any *_L / *_R\n",
    "        has_L = \"L\" in df_mouse.columns\n",
    "        has_R = \"R\" in df_mouse.columns\n",
    "        if not (has_L or has_R):\n",
    "            for c in df_mouse.columns:\n",
    "                if c.endswith(\"_L\") and not has_L:\n",
    "                    df_mouse[\"L\"] = df_mouse[c]; has_L = True\n",
    "                if c.endswith(\"_R\") and not has_R:\n",
    "                    df_mouse[\"R\"] = df_mouse[c]; has_R = True\n",
    "        if not (has_L or has_R):\n",
    "            print(f\"[WARN] {sheet_name}: no L/R columns; skipping.\")\n",
    "            continue\n",
    "\n",
    "        mouse_id = str(sheet_name)\n",
    "        assign_L, assign_R = defaultdict(list), defaultdict(list)\n",
    "\n",
    "        for _, row in df_mouse.iterrows():\n",
    "            path = str(row[\"structure_id_path\"]).strip(\"/\")\n",
    "            if not path:\n",
    "                continue\n",
    "            region_id = path.split(\"/\")[-1]  # leaf id as string\n",
    "            cid = structure_to_collapsed.get(region_id)\n",
    "            if not cid:\n",
    "                continue\n",
    "\n",
    "            if has_L and pd.notna(row[\"L\"]):\n",
    "                vL = float(row[\"L\"])\n",
    "                assign_L[cid].append(vL)\n",
    "                log_rows.append({\n",
    "                    \"mouse\": mouse_id, \"hemisphere\": \"L\",\n",
    "                    \"collapsed_region_id\": cid,\n",
    "                    \"collapsed_region_name\": id_to_name.get(cid, \"\"),\n",
    "                    \"child_region_id\": region_id,\n",
    "                    \"child_structure_id_path\": row[\"structure_id_path\"],\n",
    "                    \"cells_per_mm3_contribution\": vL\n",
    "                })\n",
    "            if has_R and pd.notna(row[\"R\"]):\n",
    "                vR = float(row[\"R\"])\n",
    "                assign_R[cid].append(vR)\n",
    "                log_rows.append({\n",
    "                    \"mouse\": mouse_id, \"hemisphere\": \"R\",\n",
    "                    \"collapsed_region_id\": cid,\n",
    "                    \"collapsed_region_name\": id_to_name.get(cid, \"\"),\n",
    "                    \"child_region_id\": region_id,\n",
    "                    \"child_structure_id_path\": row[\"structure_id_path\"],\n",
    "                    \"cells_per_mm3_contribution\": vR\n",
    "                })\n",
    "\n",
    "        # reduce to mean/std/sem per collapsed region\n",
    "        for cid, vals in assign_L.items():\n",
    "            arr = np.asarray(vals, float)\n",
    "            m = float(np.mean(arr))\n",
    "            s = float(np.std(arr, ddof=1)) if arr.size > 1 else 0.0\n",
    "            e = float(s / np.sqrt(arr.size)) if arr.size > 1 else 0.0\n",
    "            mean_L[mouse_id][cid] = m; std_L[mouse_id][cid] = s; sem_L[mouse_id][cid] = e\n",
    "\n",
    "        for cid, vals in assign_R.items():\n",
    "            arr = np.asarray(vals, float)\n",
    "            m = float(np.mean(arr))\n",
    "            s = float(np.std(arr, ddof=1)) if arr.size > 1 else 0.0\n",
    "            e = float(s / np.sqrt(arr.size)) if arr.size > 1 else 0.0\n",
    "            mean_R[mouse_id][cid] = m; std_R[mouse_id][cid] = s; sem_R[mouse_id][cid] = e\n",
    "\n",
    "    return (mean_L, std_L, sem_L), (mean_R, std_R, sem_R), log_rows\n",
    "\n",
    "def build_wide_with_meta(mean_L, mean_R, id_to_name, id_to_path, id_to_acronym):\n",
    "    \"\"\"Build a wide table: meta columns + one column per mouse hemisphere (<mouse>_L / <mouse>_R).\"\"\"\n",
    "    cols = {}\n",
    "    all_mice = sorted(set(mean_L.keys()) | set(mean_R.keys()))\n",
    "    for m in all_mice:\n",
    "        if m in mean_L and mean_L[m]:\n",
    "            cols[f\"{m}_L\"] = mean_L[m]   # dict: collapsed_id -> value\n",
    "        if m in mean_R and mean_R[m]:\n",
    "            cols[f\"{m}_R\"] = mean_R[m]\n",
    "\n",
    "    df = pd.DataFrame(cols)\n",
    "    df.index.name = \"collapsed_region_id\"\n",
    "\n",
    "    # attach metadata\n",
    "    df[\"region_id\"] = df.index.astype(str)  # keep as string key for maps\n",
    "    df[\"acronym\"] = df[\"region_id\"].map(lambda x: id_to_acronym.get(x, \"\"))\n",
    "    df[\"name\"] = df[\"region_id\"].map(lambda x: id_to_name.get(x, \"\"))\n",
    "    df[\"structure_id_path\"] = df[\"region_id\"].map(lambda x: id_to_path.get(x, \"\"))\n",
    "    df[\"depth\"] = df[\"structure_id_path\"].map(lambda p: len(str(p).strip(\"/\").split(\"/\")))\n",
    "\n",
    "    # if you prefer integer region_id in output, coerce safely\n",
    "    def to_int_maybe(x):\n",
    "        try: return int(x)\n",
    "        except: return x\n",
    "    df[\"region_id\"] = df[\"region_id\"].map(to_int_maybe)\n",
    "\n",
    "    meta_cols = [\"region_id\", \"acronym\", \"name\", \"structure_id_path\", \"depth\"]\n",
    "    ordered = meta_cols + [c for c in df.columns if c not in meta_cols]\n",
    "    return df[ordered].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -------------------- Audit Exports (safe) --------------------\n",
    "def export_audit_files(collapsed_region_ids, id_to_name, id_to_acronym, id_to_path,\n",
    "                       structure_to_collapsed, audit_dir: str):\n",
    "    \"\"\"Write: collapsed_selection.csv, child_to_collapsed_map.csv, unmapped_regions.csv. Safe if empty selection.\"\"\"\n",
    "    out_dir = Path(audit_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sel_csv  = out_dir / \"collapsed_selection.csv\"\n",
    "    map_csv  = out_dir / \"child_to_collapsed_map.csv\"\n",
    "    unmap_csv= out_dir / \"unmapped_regions.csv\"\n",
    "\n",
    "    # 1) collapsed_selection.csv\n",
    "    rows = []\n",
    "    for cid in collapsed_region_ids:\n",
    "        sid = str(cid)\n",
    "        rows.append({\n",
    "            \"region_id\": int(sid) if sid.isdigit() else sid,\n",
    "            \"acronym\":   id_to_acronym.get(sid, \"\"),\n",
    "            \"name\":      id_to_name.get(sid, \"\"),\n",
    "            \"structure_id_path\": id_to_path.get(sid, \"\"),\n",
    "            \"depth\":     node_depth_from_path(sid, id_to_path)\n",
    "        })\n",
    "    sel_df = pd.DataFrame(rows)\n",
    "    if not sel_df.empty:\n",
    "        sel_df.sort_values([\"depth\",\"region_id\"]).to_csv(sel_csv, index=False)\n",
    "    else:\n",
    "        # write headers so downstream tools don't choke\n",
    "        pd.DataFrame(columns=[\"region_id\",\"acronym\",\"name\",\"structure_id_path\",\"depth\"]).to_csv(sel_csv, index=False)\n",
    "\n",
    "    # 2) child_to_collapsed_map.csv\n",
    "    map_rows, unmapped = [], []\n",
    "    selected_set = set(map(str, collapsed_region_ids))\n",
    "    for sid, pth in id_to_path.items():\n",
    "        parts = [p for p in str(pth).strip(\"/\").split(\"/\") if p]\n",
    "        mapped = next((p for p in reversed(parts) if p in selected_set), None)\n",
    "        if mapped is None:\n",
    "            unmapped.append({\n",
    "                \"child_region_id\": int(sid) if sid.isdigit() else sid,\n",
    "                \"child_name\": id_to_name.get(sid, \"\"),\n",
    "                \"child_acronym\": id_to_acronym.get(sid, \"\"),\n",
    "                \"child_structure_id_path\": pth\n",
    "            })\n",
    "        else:\n",
    "            map_rows.append({\n",
    "                \"child_region_id\": int(sid) if sid.isdigit() else sid,\n",
    "                \"child_name\": id_to_name.get(sid, \"\"),\n",
    "                \"child_acronym\": id_to_acronym.get(sid, \"\"),\n",
    "                \"child_structure_id_path\": pth,\n",
    "                \"collapsed_region_id\": int(mapped) if mapped.isdigit() else mapped,\n",
    "                \"collapsed_name\": id_to_name.get(mapped, \"\"),\n",
    "                \"collapsed_acronym\": id_to_acronym.get(mapped, \"\"),\n",
    "                \"collapsed_structure_id_path\": id_to_path.get(mapped, \"\"),\n",
    "                \"collapsed_depth\": node_depth_from_path(mapped, id_to_path)\n",
    "            })\n",
    "    pd.DataFrame(map_rows).to_csv(map_csv, index=False)\n",
    "    pd.DataFrame(unmapped).to_csv(unmap_csv, index=False)\n",
    "\n",
    "    print(f\"[AUDIT] collapsed_selection → {sel_csv} ({len(sel_df)} rows)\")\n",
    "    print(f\"[AUDIT] child_to_collapsed_map → {map_csv} ({len(map_rows)} rows)\")\n",
    "    if unmapped:\n",
    "        print(f\"[AUDIT] unmapped_regions → {unmap_csv}  (count={len(unmapped)})\")\n",
    "    else:\n",
    "        print(\"[AUDIT] All regions mapped.\")\n",
    "\n",
    "\n",
    "# ------------------------- MAIN -------------------------\n",
    "def main():\n",
    "    # 1) Load structures & build graph\n",
    "    S, id_to_name, id_to_path, id_to_acronym = load_structures(structures_path)\n",
    "    G = build_graph_from_paths(S)\n",
    "    if ROOT_ID not in G:\n",
    "        raise ValueError(f\"Root id {ROOT_ID} not in graph.\")\n",
    "\n",
    "    # Diagnostics: ensure your prefixes actually match nodes\n",
    "    diagnose_categories(id_to_path, category_map)\n",
    "\n",
    "    # 2) Select collapsed regions **within categories** (never above them)\n",
    "    # BAN_IDS should include \"8\" to avoid the shallow hub\n",
    "    BAN_IDS = {\"8\"}\n",
    "\n",
    "    collapsed_region_ids = select_frontier_antichain_within_categories(\n",
    "        G,\n",
    "        id_to_path=id_to_path,\n",
    "        category_map=category_map,   # your prefixes, canonicalized elsewhere\n",
    "        target_n=TARGET_N,\n",
    "        root_id=\"997\",\n",
    "        min_offset=1,                # start one level below category root\n",
    "        max_extra_depth=3,           # cap depth to root_depth + 3  (tune: 2..4)\n",
    "        ban_ids=BAN_IDS,\n",
    "    )\n",
    "\n",
    "    # Sanity: these should be mid-depth ancestors, not last leaves\n",
    "    depths = [max(len(id_to_path[s].strip(\"/\").split(\"/\")) - 1, 0) for s in collapsed_region_ids]\n",
    "    print(f\"[SELECT] {len(collapsed_region_ids)} nodes, depth {min(depths)}–{max(depths)}\")\n",
    "\n",
    "\n",
    "    # Fallback if empty or too small: allow category roots\n",
    "    #if len(collapsed_region_ids) == 0:\n",
    "    #    print(\"[WARN] No nodes selected under current constraints. Retrying allowing category roots...\")\n",
    "    #    collapsed_region_ids = select_collapsed_antichain_within_categories(\n",
    "    #        G, id_to_path, category_map, TARGET_N, root_id=ROOT_ID,\n",
    "    #        min_offset=0, allow_category_roots=True, ban_ids=BAN_IDS, relax=True\n",
    "    #    )\n",
    "\n",
    "    depths = [node_depth_from_path(s, id_to_path) for s in collapsed_region_ids] if collapsed_region_ids else []\n",
    "    if collapsed_region_ids:\n",
    "        print(f\"[SELECT] Selected {len(collapsed_region_ids)} collapsed nodes within categories. \"\n",
    "              f\"Depth range: {min(depths)}–{max(depths)}\")\n",
    "    else:\n",
    "        print(\"[SELECT] Still selected 0 nodes — check category prefixes in the DIAG output above.\")\n",
    "\n",
    "    # 3) Map every atlas region to its nearest selected ancestor (will be empty map if selection empty)\n",
    "    structure_to_collapsed = map_child_to_collapsed(id_to_path, collapsed_region_ids)\n",
    "\n",
    "    # 4) Aggregate per mouse & hemisphere (cells_per_mm3)\n",
    "    (mean_L, std_L, sem_L), (mean_R, std_R, sem_R), log_rows = aggregate_per_mouse_hemi(\n",
    "        input_excel, structure_to_collapsed, id_to_name, id_to_path\n",
    "    )\n",
    "\n",
    "    # 5) Build wide tables with metadata\n",
    "    if collapsed_region_ids:\n",
    "        df_mean = build_wide_with_meta(mean_L, mean_R, id_to_name, id_to_path, id_to_acronym)\n",
    "        df_std  = build_wide_with_meta(std_L,  std_R,  id_to_name, id_to_path, id_to_acronym)\n",
    "        df_sem  = build_wide_with_meta(sem_L,  sem_R,  id_to_name, id_to_path, id_to_acronym)\n",
    "    else:\n",
    "        # empty frames with just meta headers\n",
    "        df_mean = pd.DataFrame(columns=[\"region_id\",\"acronym\",\"name\",\"structure_id_path\",\"depth\"])\n",
    "        df_std  = df_mean.copy(); df_sem = df_mean.copy()\n",
    "\n",
    "    # 6) Save outputs\n",
    "    with pd.ExcelWriter(output_excel) as writer:\n",
    "        df_mean.to_excel(writer, sheet_name=\"mean_cells_per_mm3\", index=False)\n",
    "        df_std.to_excel(writer,  sheet_name=\"std_cells_per_mm3\",  index=False)\n",
    "        df_sem.to_excel(writer,  sheet_name=\"sem_cells_per_mm3\",  index=False)\n",
    "    print(f\"[OK] Collapsed matrices written → {output_excel}\")\n",
    "\n",
    "    pd.DataFrame(log_rows).to_csv(output_log, index=False)\n",
    "    print(f\"[OK] Contribution log written → {output_log} (rows={len(log_rows)})\")\n",
    "\n",
    "    # 7) Audit CSVs (safe if empty)\n",
    "    export_audit_files(collapsed_region_ids, id_to_name, id_to_acronym, id_to_path,\n",
    "                       structure_to_collapsed, audit_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "napari-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
