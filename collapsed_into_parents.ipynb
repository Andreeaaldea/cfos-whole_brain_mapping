{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb6a83c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 47 regions had no data and were excluded.\n",
      "Final matrix saved to:\n",
      "Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\collapsed_region_matrix.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "structures_csv = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\code\\cfos_preprocessing\\allen_mouse_10um_v1.2\\structures.csv\"\n",
    "category_csv = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\pe_grouped.csv\"\n",
    "input_excel = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\mean_dff_per_region_by_mouse_brainmapper.xlsx\"\n",
    "output_file = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\collapsed_region_matrix.xlsx\"\n",
    "target_n_regions = 160\n",
    "\n",
    "# === LOAD REGION HIERARCHY ===\n",
    "region_table = pd.read_csv(structures_csv)\n",
    "region_table['structure_id_path'] = region_table['structure_id_path'].astype(str)\n",
    "\n",
    "# Build graph from structure_id_path\n",
    "G = nx.DiGraph()\n",
    "for _, row in region_table.iterrows():\n",
    "    path = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "    for i in range(len(path) - 1):\n",
    "        G.add_edge(path[i], path[i + 1])\n",
    "\n",
    "# === COLLAPSE TREE TO ~130 REGIONS (STRUCTURE ONLY) ===\n",
    "def collapse_tree_to_n(graph, n_regions):\n",
    "    leaves = [node for node in graph.nodes if graph.out_degree(node) == 0]\n",
    "    all_paths = [nx.shortest_path(graph, source=\"997\", target=leaf) for leaf in leaves if nx.has_path(graph, \"997\", leaf)]\n",
    "    from collections import Counter\n",
    "    counts = Counter()\n",
    "    for path in all_paths:\n",
    "        counts.update(path)\n",
    "    ranked = [node for node, _ in counts.most_common()]\n",
    "    selected = set()\n",
    "    for node in ranked:\n",
    "        descendants = nx.descendants(graph, node)\n",
    "        if not any(d in selected for d in descendants):\n",
    "            selected.add(node)\n",
    "        if len(selected) >= n_regions:\n",
    "            break\n",
    "    return list(selected)\n",
    "\n",
    "collapsed_region_ids = collapse_tree_to_n(G, target_n_regions)\n",
    "\n",
    "\n",
    "# === MAP ALL STRUCTURE IDs TO THEIR COLLAPSED TARGET REGION ===\n",
    "structure_to_target = {}\n",
    "for _, row in region_table.iterrows():\n",
    "    sid = str(row[\"id\"])\n",
    "    path = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "    for node in reversed(path):\n",
    "        if node in collapsed_region_ids:\n",
    "            structure_to_target[sid] = node\n",
    "            break\n",
    "\n",
    "# === LOAD AND MAP DFF DATA FROM ALL MICE ===\n",
    "df_by_mouse = pd.read_excel(input_excel, sheet_name=None)\n",
    "matrix_data = {}\n",
    "\n",
    "for sheet_name, df_mouse in df_by_mouse.items():\n",
    "    if sheet_name.lower() == \"summary\" or \"structure_id_path\" not in df_mouse.columns:\n",
    "        continue\n",
    "\n",
    "    region_means = {}\n",
    "    for _, row in df_mouse.iterrows():\n",
    "        path = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "        leaf_id = path[-1]\n",
    "        target = structure_to_target.get(leaf_id)\n",
    "        if target:\n",
    "            region_means.setdefault(target, []).append(row[\"mean_dff\"])\n",
    "\n",
    "    averaged = {rid: sum(vals) / len(vals) for rid, vals in region_means.items()}\n",
    "    matrix_data[sheet_name] = averaged\n",
    "\n",
    "\n",
    "# === ASSEMBLE WIDE MATRIX ===\n",
    "df_matrix = pd.DataFrame(matrix_data).T  # Mice × Regions\n",
    "available_regions = [rid for rid in collapsed_region_ids if rid in df_matrix.columns]\n",
    "missing = set(collapsed_region_ids) - set(df_matrix.columns)\n",
    "print(f\"[INFO] {len(missing)} regions had no data and were excluded.\")\n",
    "df_matrix = df_matrix[available_regions]\n",
    "df_matrix = df_matrix.T  # Regions × Mice\n",
    "\n",
    "# === ADD REGION INFO: acronym, name, depth ===\n",
    "id_to_meta = region_table.set_index(\"id\")[[\"acronym\", \"name\", \"structure_id_path\"]].astype(str)\n",
    "df_matrix[\"acronym\"] = df_matrix.index.map(lambda x: id_to_meta.loc[int(x), \"acronym\"] if int(x) in id_to_meta.index else \"\")\n",
    "df_matrix[\"name\"] = df_matrix.index.map(lambda x: id_to_meta.loc[int(x), \"name\"] if int(x) in id_to_meta.index else \"\")\n",
    "df_matrix[\"depth\"] = df_matrix.index.map(lambda x: len(id_to_meta.loc[int(x), \"structure_id_path\"].strip(\"/\").split(\"/\")) if int(x) in id_to_meta.index else None)\n",
    "df_matrix.insert(0, \"region_id\", df_matrix.index)\n",
    "\n",
    "# === ADD CATEGORY FROM pe_grouped.csv ===\n",
    "category_df = pd.read_csv(category_csv)\n",
    "category_df[\"abbrev\"] = category_df[\"abbrev\"].astype(str)\n",
    "category_df[\"name\"] = category_df[\"name\"].astype(str)\n",
    "\n",
    "df_matrix[\"category\"] = df_matrix[\"acronym\"].map(dict(zip(category_df[\"abbrev\"], category_df[\"category\"])))\n",
    "unmatched = df_matrix[\"category\"].isna()\n",
    "df_matrix.loc[unmatched, \"category\"] = df_matrix.loc[unmatched, \"name\"].map(\n",
    "    dict(zip(category_df[\"name\"], category_df[\"category\"]))\n",
    ")\n",
    "\n",
    "# === SAVE ===\n",
    "df_matrix.to_excel(output_file, index=False)\n",
    "print(f\"Final matrix saved to:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "354\tMedulla\t/997/8/343/1065/354/\n",
    "771\tPons\t/997/8/343/1065/771/\n",
    "1097\tHypothalamus\t/997/8/343/1129/1097/\n",
    "549\tThalamus\t/997/8/343/1129/549/\n",
    "313\tMidbrain\t/997/8/343/313/\n",
    "512\tCerebellum\t/997/8/512/\n",
    "695\tCortical plate\t/997/8/567/688/695/\n",
    "703\tCortical subplate\t/997/8/567/688/703/\n",
    "803\tPallidum\t/997/8/567/623/803/\n",
    "477\tStriatum\t/997/8/567/623/477/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56410d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reduced to 134 non-overlapping regions.\n",
      "[INFO] 18 collapsed regions had no data.\n",
      " Final matrix saved to:\n",
      "Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\colapsed_dff_cleaned_210.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "structures_csv = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\code\\cfos_preprocessing\\allen_mouse_10um_v1.2\\structures.csv\"\n",
    "input_excel = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\mean_dff_per_region_by_mouse_brainmapper.xlsx\"\n",
    "output_file = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\colapsed_dff_cleaned_210.xlsx\"\n",
    "target_n_regions = 210\n",
    "\n",
    "# === LOAD REGION HIERARCHY ===\n",
    "region_table = pd.read_csv(structures_csv)\n",
    "region_table['structure_id_path'] = region_table['structure_id_path'].astype(str)\n",
    "id_to_path = region_table.set_index(\"id\")[\"structure_id_path\"].astype(str).to_dict()\n",
    "\n",
    "# Build anatomical graph\n",
    "G = nx.DiGraph()\n",
    "for _, row in region_table.iterrows():\n",
    "    path = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "    for i in range(len(path) - 1):\n",
    "        G.add_edge(path[i], path[i + 1])\n",
    "\n",
    "# Collapse to 130 nodes\n",
    "def collapse_tree_to_n(graph, n_regions):\n",
    "    leaves = [node for node in graph.nodes if graph.out_degree(node) == 0]\n",
    "    all_paths = [nx.shortest_path(graph, source=\"997\", target=leaf) for leaf in leaves if nx.has_path(graph, \"997\", leaf)]\n",
    "    from collections import Counter\n",
    "    counts = Counter()\n",
    "    for path in all_paths:\n",
    "        counts.update(path)\n",
    "    ranked = [node for node, _ in counts.most_common()]\n",
    "    selected = set()\n",
    "    for node in ranked:\n",
    "        descendants = nx.descendants(graph, node)\n",
    "        if not any(d in selected for d in descendants):\n",
    "            selected.add(node)\n",
    "        if len(selected) >= n_regions:\n",
    "            break\n",
    "    return list(selected)\n",
    "\n",
    "collapsed_region_ids = collapse_tree_to_n(G, target_n_regions)\n",
    "\n",
    "# === REMOVE REDUNDANT PARENT REGIONS ===\n",
    "collapsed_with_paths = [(rid, id_to_path.get(int(rid), \"\")) for rid in collapsed_region_ids]\n",
    "\n",
    "def is_subpath(p1, p2):\n",
    "    return p2.startswith(p1) and p1 != p2\n",
    "\n",
    "filtered_regions = []\n",
    "paths = [p for _, p in collapsed_with_paths]\n",
    "for i, (rid1, path1) in enumerate(collapsed_with_paths):\n",
    "    if any(is_subpath(path1, p2) for j, p2 in enumerate(paths) if j != i):\n",
    "        continue\n",
    "    filtered_regions.append(rid1)\n",
    "\n",
    "collapsed_region_ids = filtered_regions\n",
    "print(f\"[INFO] Reduced to {len(collapsed_region_ids)} non-overlapping regions.\")\n",
    "\n",
    "# === MAP STRUCTURE TO COLLAPSED REGIONS ===\n",
    "structure_to_target = {}\n",
    "for _, row in region_table.iterrows():\n",
    "    sid = str(row[\"id\"])\n",
    "    path = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "    for node in reversed(path):\n",
    "        if node in collapsed_region_ids:\n",
    "            structure_to_target[sid] = node\n",
    "            break\n",
    "\n",
    "# === LOAD MOUSE DFF ===\n",
    "df_by_mouse = pd.read_excel(input_excel, sheet_name=None)\n",
    "matrix_data = {}\n",
    "\n",
    "for sheet_name, df_mouse in df_by_mouse.items():\n",
    "    if sheet_name.lower() == \"summary\" or \"structure_id_path\" not in df_mouse.columns:\n",
    "        continue\n",
    "    region_means = {}\n",
    "    for _, row in df_mouse.iterrows():\n",
    "        path = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "        leaf_id = path[-1]\n",
    "        target = structure_to_target.get(leaf_id)\n",
    "        if target:\n",
    "            region_means.setdefault(target, []).append(row[\"mean_dff\"])\n",
    "    averaged = {rid: sum(vals) / len(vals) for rid, vals in region_means.items()}\n",
    "    matrix_data[sheet_name] = averaged\n",
    "\n",
    "# === FINAL MATRIX ===\n",
    "df_matrix = pd.DataFrame(matrix_data).T\n",
    "available_regions = [rid for rid in collapsed_region_ids if rid in df_matrix.columns]\n",
    "missing = set(collapsed_region_ids) - set(df_matrix.columns)\n",
    "print(f\"[INFO] {len(missing)} collapsed regions had no data.\")\n",
    "\n",
    "df_matrix = df_matrix[available_regions].T\n",
    "df_matrix[\"region_id\"] = df_matrix.index.astype(int)\n",
    "\n",
    "# Add structure metadata\n",
    "id_meta = region_table.set_index(\"id\")[[\"acronym\", \"name\", \"structure_id_path\"]].astype(str)\n",
    "df_matrix[\"acronym\"] = df_matrix[\"region_id\"].map(lambda x: id_meta.loc[x, \"acronym\"] if x in id_meta.index else \"\")\n",
    "df_matrix[\"name\"] = df_matrix[\"region_id\"].map(lambda x: id_meta.loc[x, \"name\"] if x in id_meta.index else \"\")\n",
    "df_matrix[\"structure_id_path\"] = df_matrix[\"region_id\"].map(lambda x: id_meta.loc[x, \"structure_id_path\"] if x in id_meta.index else \"\")\n",
    "df_matrix[\"depth\"] = df_matrix[\"structure_id_path\"].map(lambda p: len(p.strip(\"/\").split(\"/\")))\n",
    "\n",
    "# Add anatomical category based on path prefix\n",
    "category_map = {\n",
    "    \"Medulla\": \"/997/8/343/1065/354/\",\n",
    "    \"Pons\": \"/997/8/343/1065/771/\",\n",
    "    \"Hypothalamus\": \"/997/8/343/1129/1097/\",\n",
    "    \"Thalamus\": \"/997/8/343/1129/549/\",\n",
    "    \"Midbrain\": \"/997/8/343/313/\",\n",
    "    \"Cerebellum\": \"/997/8/512/\",\n",
    "    \"Cortical plate\": \"/997/8/567/688/695/\",\n",
    "    \"Cortical subplate\": \"/997/8/567/688/703/\",\n",
    "    \"Pallidum\": \"/997/8/567/623/803/\",\n",
    "    \"Striatum\": \"/997/8/567/623/477/\"\n",
    "}\n",
    "\n",
    "def assign_category(path):\n",
    "    for cat, prefix in category_map.items():\n",
    "        if path.startswith(prefix):\n",
    "            return cat\n",
    "    return \"Other\"\n",
    "\n",
    "df_matrix[\"category\"] = df_matrix[\"structure_id_path\"].map(assign_category)\n",
    "\n",
    "# Reorder columns\n",
    "meta_cols = [\"region_id\", \"acronym\", \"name\", \"structure_id_path\", \"depth\", \"category\"]\n",
    "df_matrix = df_matrix[meta_cols + [col for col in df_matrix.columns if col not in meta_cols]]\n",
    "\n",
    "# === SAVE ===\n",
    "df_matrix.to_excel(output_file, index=False)\n",
    "print(f\" Final matrix saved to:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c733449a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped 0 regions to collapsed parents.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "Saved to: Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\child_to_collapsed_map.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === CONFIG ===\n",
    "structures_path = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\code\\cfos_preprocessing\\allen_mouse_10um_v1.2\\structures.csv\"\n",
    "output_csv = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\child_to_collapsed_map.csv\"\n",
    "\n",
    "# === Load region hierarchy ===\n",
    "region_table = pd.read_csv(structures_path)\n",
    "region_table[\"structure_id_path\"] = region_table[\"structure_id_path\"].astype(str)\n",
    "\n",
    "id_to_path = region_table.set_index(\"id\")[\"structure_id_path\"].astype(str).to_dict()\n",
    "id_to_name = region_table.set_index(\"id\")[\"name\"].astype(str).to_dict()\n",
    "\n",
    "# === Define the 10 collapsed categories (based on root IDs) ===\n",
    "collapsed_region_ids = [\n",
    "    354,   # Medulla\n",
    "    771,   # Pons\n",
    "    1097,  # Hypothalamus\n",
    "    549,   # Thalamus\n",
    "    313,   # Midbrain\n",
    "    512,   # Cerebellum\n",
    "    695,   # Cortical plate\n",
    "    703,   # Cortical subplate\n",
    "    803,   # Pallidum\n",
    "    477    # Striatum\n",
    "]\n",
    "collapsed_region_ids = list(map(str, collapsed_region_ids))\n",
    "\n",
    "# === Build structure_id_path mapping ===\n",
    "collapsed_paths = {cid: id_to_path[int(cid)] for cid in collapsed_region_ids}\n",
    "\n",
    "# === Map each region to its collapsed parent ===\n",
    "mapping_rows = []\n",
    "for sid, path in id_to_path.items():\n",
    "    assigned_parent = None\n",
    "    for parent_id, parent_path in collapsed_paths.items():\n",
    "        parent_path_clean = parent_path.strip(\"/\")\n",
    "        if path.startswith(parent_path_clean + \"/\") or path == parent_path_clean:\n",
    "            assigned_parent = parent_id\n",
    "            break\n",
    "\n",
    "    if assigned_parent:\n",
    "        mapping_rows.append({\n",
    "            \"child_region_id\": sid,\n",
    "            \"child_region_name\": id_to_name.get(int(sid), \"\"),\n",
    "            \"child_structure_id_path\": path,\n",
    "            \"collapsed_region_id\": assigned_parent,\n",
    "            \"collapsed_region_name\": id_to_name.get(int(assigned_parent), \"\")\n",
    "        })\n",
    "\n",
    "df_map = pd.DataFrame(mapping_rows)\n",
    "\n",
    "# === Save or display ===\n",
    "print(f\"Mapped {len(df_map)} regions to collapsed parents.\")\n",
    "print(df_map.head(10))\n",
    "\n",
    "df_map.to_csv(output_csv, index=False)\n",
    "print(f\"Saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "954cfb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsed matrix saved to:\n",
      "Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\final_collapsed_matrix.xlsx\n",
      "Collapse log with ΔF/F contributions saved to:\n",
      "Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\dff_collapsing_log.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# === CONFIG ===\n",
    "structures_path = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\code\\cfos_preprocessing\\allen_mouse_10um_v1.2\\structures.csv\"\n",
    "input_excel = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\mean_dff_per_region_by_mouse_brainmapper.xlsx\"\n",
    "output_matrix = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\final_collapsed_matrix.xlsx\"\n",
    "output_log = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\dff_collapsing_log.csv\"\n",
    "TARGET_N = 160\n",
    "\n",
    "# === Load hierarchy ===\n",
    "region_table = pd.read_csv(structures_path)\n",
    "region_table[\"structure_id_path\"] = region_table[\"structure_id_path\"].astype(str)\n",
    "id_to_path = region_table.set_index(\"id\")[\"structure_id_path\"].astype(str).to_dict()\n",
    "id_to_name = region_table.set_index(\"id\")[\"name\"].astype(str).to_dict()\n",
    "\n",
    "# === Build graph from structure_id_path ===\n",
    "G = nx.DiGraph()\n",
    "for _, row in region_table.iterrows():\n",
    "    path = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "    for i in range(len(path) - 1):\n",
    "        G.add_edge(path[i], path[i + 1])\n",
    "\n",
    "# === Collapse function ===\n",
    "def collapse_tree_to_n(graph, n):\n",
    "    leaves = [n for n in graph.nodes if graph.out_degree(n) == 0]\n",
    "    all_paths = [nx.shortest_path(graph, source=\"997\", target=leaf)\n",
    "                 for leaf in leaves if nx.has_path(graph, \"997\", leaf)]\n",
    "    from collections import Counter\n",
    "    counts = Counter()\n",
    "    for path in all_paths:\n",
    "        counts.update(path)\n",
    "    ranked = [node for node, _ in counts.most_common()]\n",
    "    selected = set()\n",
    "    for node in ranked:\n",
    "        descendants = nx.descendants(graph, node)\n",
    "        if not any(d in selected for d in descendants):\n",
    "            selected.add(node)\n",
    "        if len(selected) >= n:\n",
    "            break\n",
    "    return list(selected)\n",
    "\n",
    "collapsed_region_ids = collapse_tree_to_n(G, TARGET_N)\n",
    "\n",
    "# === Map child → parent ===\n",
    "structure_to_collapsed = {}\n",
    "for sid, path in id_to_path.items():\n",
    "    parts = path.strip(\"/\").split(\"/\")\n",
    "    for node in reversed(parts):\n",
    "        if node in collapsed_region_ids:\n",
    "            structure_to_collapsed[str(sid)] = node\n",
    "            break\n",
    "\n",
    "# === Load per-mouse DFF ===\n",
    "df_by_mouse = pd.read_excel(input_excel, sheet_name=None)\n",
    "matrix_data = defaultdict(dict)\n",
    "log_rows = []\n",
    "\n",
    "for sheet_name, df_mouse in df_by_mouse.items():\n",
    "    if sheet_name.lower() == \"summary\" or \"structure_id_path\" not in df_mouse.columns:\n",
    "        continue\n",
    "    mouse_id = sheet_name\n",
    "    temp_assignments = defaultdict(list)\n",
    "\n",
    "    for _, row in df_mouse.iterrows():\n",
    "        path = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "        region_id = path[-1]\n",
    "        collapsed_id = structure_to_collapsed.get(region_id)\n",
    "        if collapsed_id:\n",
    "            temp_assignments[collapsed_id].append((region_id, row[\"mean_dff\"]))\n",
    "\n",
    "            # Logging for traceability\n",
    "            log_rows.append({\n",
    "                \"mouse\": mouse_id,\n",
    "                \"collapsed_region_id\": collapsed_id,\n",
    "                \"collapsed_region_name\": id_to_name.get(int(collapsed_id), \"\"),\n",
    "                \"child_region_id\": region_id,\n",
    "                \"child_region_name\": id_to_name.get(int(region_id), \"\"),\n",
    "                \"child_structure_id_path\": row[\"structure_id_path\"],\n",
    "                \"mean_dff_contribution\": row[\"mean_dff\"]\n",
    "            })\n",
    "\n",
    "    for cid, values in temp_assignments.items():\n",
    "        mean_val = sum(v for _, v in values) / len(values)\n",
    "        matrix_data[mouse_id][cid] = mean_val\n",
    "\n",
    "# === Build matrix\n",
    "df_matrix = pd.DataFrame(matrix_data).T\n",
    "df_matrix.index.name = \"mouse\"\n",
    "df_matrix.columns.name = \"collapsed_region_id\"\n",
    "df_matrix = df_matrix.T\n",
    "df_matrix[\"region_id\"] = df_matrix.index.astype(int)\n",
    "df_matrix[\"name\"] = df_matrix[\"region_id\"].map(lambda x: id_to_name.get(x, \"\"))\n",
    "df_matrix[\"structure_id_path\"] = df_matrix[\"region_id\"].map(lambda x: id_to_path.get(x, \"\"))\n",
    "df_matrix[\"depth\"] = df_matrix[\"structure_id_path\"].map(lambda p: len(p.strip(\"/\").split(\"/\")))\n",
    "\n",
    "# Move metadata columns to front\n",
    "meta_cols = [\"region_id\", \"name\", \"structure_id_path\", \"depth\"]\n",
    "df_matrix = df_matrix[meta_cols + [col for col in df_matrix.columns if col not in meta_cols]]\n",
    "\n",
    "# === Save\n",
    "df_matrix.to_excel(output_matrix, index=False)\n",
    "print(f\"Collapsed matrix saved to:\\n{output_matrix}\")\n",
    "\n",
    "log_df = pd.DataFrame(log_rows)\n",
    "log_df.to_csv(output_log, index=False)\n",
    "print(f\"Collapse log with ΔF/F contributions saved to:\\n{output_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24604412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix with mean, std, sem saved to:\n",
      "Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\final_collapsed_matrix_2.xlsx\n",
      "Detailed ΔF/F log saved to:\n",
      "Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\dff_downsampled\\dff_collapsing_log_2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# === CONFIG ===\n",
    "structures_path = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\\\structures.csv\"\n",
    "input_excel = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\per_mouse_sheets.xlsx\"\n",
    "output_excel = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsed_matrix.xlsx\"\n",
    "output_log = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsing_log.csv\"\n",
    "TARGET_N = 160\n",
    "\n",
    "# === Load Allen structure data ===\n",
    "region_table = pd.read_csv(structures_path)\n",
    "region_table[\"id\"] = region_table[\"id\"].astype(str)\n",
    "region_table[\"structure_id_path\"] = region_table[\"structure_id_path\"].astype(str)\n",
    "\n",
    "id_to_acronym = region_table.set_index(\"id\")[\"acronym\"].astype(str).to_dict()\n",
    "id_to_path    = region_table.set_index(\"id\")[\"structure_id_path\"].astype(str).to_dict()\n",
    "id_to_name    = region_table.set_index(\"id\")[\"name\"].astype(str).to_dict()\n",
    "\n",
    "# === Build anatomical hierarchy graph (string node IDs) ===\n",
    "G = nx.DiGraph()\n",
    "for _, row in region_table.iterrows():\n",
    "    parts = [p for p in row[\"structure_id_path\"].strip(\"/\").split(\"/\") if p]\n",
    "    for i in range(len(parts) - 1):\n",
    "        G.add_edge(parts[i], parts[i + 1])\n",
    "\n",
    "\n",
    "# === Collapse region selection ===\n",
    "def collapse_tree_to_n(graph, n):\n",
    "    leaves = [n for n in graph.nodes if graph.out_degree(n) == 0]\n",
    "    all_paths = [nx.shortest_path(graph, source=\"997\", target=leaf)\n",
    "                 for leaf in leaves if nx.has_path(graph, \"997\", leaf)]\n",
    "    from collections import Counter\n",
    "    counts = Counter()\n",
    "    for path in all_paths:\n",
    "        counts.update(path)\n",
    "    ranked = [node for node, _ in counts.most_common()]\n",
    "    selected = set()\n",
    "    for node in ranked:\n",
    "        descendants = nx.descendants(graph, node)\n",
    "        if not any(d in selected for d in descendants):\n",
    "            selected.add(node)\n",
    "        if len(selected) >= n:\n",
    "            break\n",
    "    return list(selected)\n",
    "\n",
    "collapsed_region_ids = collapse_tree_to_n(G, TARGET_N)\n",
    "\n",
    "# === Map child → collapsed parent ===\n",
    "structure_to_collapsed = {}\n",
    "for sid, path in id_to_path.items():\n",
    "    parts = path.strip(\"/\").split(\"/\")\n",
    "    for node in reversed(parts):\n",
    "        if node in collapsed_region_ids:\n",
    "            structure_to_collapsed[str(sid)] = node\n",
    "            break\n",
    "\n",
    "# === Process all mice ===\n",
    "df_by_mouse = pd.read_excel(input_excel, sheet_name=None)\n",
    "mean_data = defaultdict(dict)\n",
    "std_data = defaultdict(dict)\n",
    "sem_data = defaultdict(dict)\n",
    "log_rows = []\n",
    "\n",
    "for sheet_name, df_mouse in df_by_mouse.items():\n",
    "    if sheet_name.lower() == \"summary\" or \"structure_id_path\" not in df_mouse.columns:\n",
    "        continue\n",
    "    mouse_id = sheet_name\n",
    "    temp_assignments = defaultdict(list)\n",
    "\n",
    "    for _, row in df_mouse.iterrows():\n",
    "        path = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "        region_id = path[-1]\n",
    "        collapsed_id = structure_to_collapsed.get(region_id)\n",
    "        if collapsed_id:\n",
    "            temp_assignments[collapsed_id].append(row[\"mean_dff\"])\n",
    "            log_rows.append({\n",
    "                \"mouse\": mouse_id,\n",
    "                \"collapsed_region_id\": collapsed_id,\n",
    "                \"collapsed_region_name\": id_to_name.get(int(collapsed_id), \"\"),\n",
    "                \"child_region_id\": region_id,\n",
    "                \"child_region_name\": id_to_name.get(int(region_id), \"\"),\n",
    "                \"child_structure_id_path\": row[\"structure_id_path\"],\n",
    "                \"mean_dff_contribution\": row[\"mean_dff\"]\n",
    "            })\n",
    "\n",
    "    for cid, values in temp_assignments.items():\n",
    "        arr = np.array(values, dtype=float)\n",
    "        mean_data[mouse_id][cid] = np.mean(arr)\n",
    "        std_data[mouse_id][cid] = np.std(arr, ddof=1) if len(arr) > 1 else 0.0\n",
    "        sem_data[mouse_id][cid] = std_data[mouse_id][cid] / np.sqrt(len(arr)) if len(arr) > 1 else 0.0\n",
    "\n",
    "# === Format outputs\n",
    "def build_df(data_dict, id_to_name, id_to_path, id_to_acronym):\n",
    "    df = pd.DataFrame(data_dict).T\n",
    "    df.index.name = \"mouse\"\n",
    "    df.columns.name = \"collapsed_region_id\"\n",
    "    df = df.T\n",
    "    df[\"region_id\"] = df.index.astype(int)\n",
    "    df[\"name\"] = df[\"region_id\"].map(lambda x: id_to_name.get(x, \"\"))\n",
    "    df[\"acronym\"] = df[\"region_id\"].map(lambda x: id_to_acronym.get(x, \"\"))\n",
    "    df[\"structure_id_path\"] = df[\"region_id\"].map(lambda x: id_to_path.get(x, \"\"))\n",
    "    df[\"depth\"] = df[\"structure_id_path\"].map(lambda p: len(p.strip(\"/\").split(\"/\")))\n",
    "    meta_cols = [\"region_id\", \"name\", \"acronym\", \"structure_id_path\", \"depth\"]\n",
    "    return df[meta_cols + [col for col in df.columns if col not in meta_cols]]\n",
    "\n",
    "df_mean = build_df(mean_data, id_to_name, id_to_path, id_to_acronym)\n",
    "df_std = build_df(std_data, id_to_name, id_to_path, id_to_acronym)\n",
    "df_sem = build_df(sem_data, id_to_name, id_to_path, id_to_acronym)\n",
    "\n",
    "# === Export all\n",
    "with pd.ExcelWriter(output_excel) as writer:\n",
    "    df_mean.to_excel(writer, sheet_name=\"mean_dff\", index=False)\n",
    "    df_std.to_excel(writer, sheet_name=\"std_dff\", index=False)\n",
    "    df_sem.to_excel(writer, sheet_name=\"sem_dff\", index=False)\n",
    "print(f\"Matrix with mean, std, sem saved to:\\n{output_excel}\")\n",
    "\n",
    "# Save log\n",
    "pd.DataFrame(log_rows).to_csv(output_log, index=False)\n",
    "print(f\"Detailed ΔF/F log saved to:\\n{output_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35d832c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsed L/R mean/std/sem saved to:\n",
      "Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsed_matrix.xlsx\n",
      "Detailed density collapsing log saved to:\n",
      "Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsing_log.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# === CONFIG ===\n",
    "structures_path = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\structures.csv\"\n",
    "input_excel     = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\per_mouse_sheets.xlsx\"\n",
    "output_excel    = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsed_matrix.xlsx\"\n",
    "output_log      = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsing_log.csv\"\n",
    "out_dir = Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "sel_csv  = out_dir / \"collapsed_selection.csv\"\n",
    "map_csv  = out_dir / \"child_to_collapsed_map.csv\"\n",
    "unmap_csv= out_dir / \"unmapped_regions.csv\"\n",
    "TARGET_N        = 160\n",
    "ROOT_ID         = \"997\"   # Allen root (mouse)\n",
    "\n",
    "# === Load Allen structure data ===\n",
    "region_table = pd.read_csv(structures_path)\n",
    "# --- after loading region_table ---\n",
    "region_table = region_table.copy()\n",
    "region_table[\"id\"] = region_table[\"id\"].astype(str)\n",
    "region_table[\"structure_id_path\"] = region_table[\"structure_id_path\"].astype(str)\n",
    "\n",
    "# maps with STRING KEYS\n",
    "id_to_name = region_table.set_index(\"id\")[\"name\"].astype(str).to_dict()\n",
    "id_to_path = region_table.set_index(\"id\")[\"structure_id_path\"].astype(str).to_dict()\n",
    "\n",
    "# build graph with STRING node ids\n",
    "G = nx.DiGraph()\n",
    "for _, row in region_table.iterrows():\n",
    "    parts = [p for p in row[\"structure_id_path\"].strip(\"/\").split(\"/\") if p]\n",
    "    for i in range(len(parts) - 1):\n",
    "        G.add_edge(parts[i], parts[i+1])\n",
    "\n",
    "def node_depth(v: str) -> int:\n",
    "    \"\"\"Depth from path (edges from root).\"\"\"\n",
    "    p = id_to_path.get(v, \"\")\n",
    "    return max(len(p.strip(\"/\").split(\"/\")) - 1, 0)\n",
    "\n",
    "def collapse_tree_to_n_antichain(graph: nx.DiGraph, n: int, root_id: str = ROOT_ID) -> list[str]:\n",
    "    \"\"\"Pick n nodes with no ancestor/descendant conflicts (antichain).\"\"\"\n",
    "    # rank by root->leaf path frequency\n",
    "    leaves = [v for v in graph.nodes if graph.out_degree(v) == 0 and nx.has_path(graph, root_id, v)]\n",
    "    counts = Counter()\n",
    "    for leaf in leaves:\n",
    "        for v in nx.shortest_path(graph, root_id, leaf):\n",
    "            counts[v] += 1\n",
    "\n",
    "    # prefer deeper nodes when ties (avoids huge parents like 549)\n",
    "    ranked = sorted(counts.keys(),\n",
    "                    key=lambda v: (counts[v], node_depth(v)),\n",
    "                    reverse=True)\n",
    "\n",
    "    selected: list[str] = []\n",
    "    selset: set[str] = set()\n",
    "    for v in ranked:\n",
    "        if v == root_id:\n",
    "            continue\n",
    "        # reject if ANY ancestor or descendant is already selected\n",
    "        if any((v in nx.descendants(graph, s)) or (s in nx.descendants(graph, v)) for s in selset):\n",
    "            continue\n",
    "        selset.add(v)\n",
    "        selected.append(v)\n",
    "        if len(selected) >= n:\n",
    "            break\n",
    "    return selected\n",
    "\n",
    "def prune_to_antichain(selected_ids: list[str], graph: nx.DiGraph) -> list[str]:\n",
    "    \"\"\"Hard prune until no ancestor/descendant pairs remain (keep the deeper node).\"\"\"\n",
    "    sel = set(map(str, selected_ids))\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        to_drop = set()\n",
    "        for u in list(sel):\n",
    "            for v in list(sel):\n",
    "                if u == v:\n",
    "                    continue\n",
    "                # u ancestor of v?\n",
    "                if v in nx.descendants(graph, u):\n",
    "                    # drop the shallower one (smaller depth)\n",
    "                    drop = u if node_depth(u) <= node_depth(v) else v\n",
    "                    to_drop.add(drop)\n",
    "        if to_drop:\n",
    "            sel -= to_drop\n",
    "            changed = True\n",
    "    return list(sel)\n",
    "\n",
    "def verify_antichain(selected_ids: list[str], graph: nx.DiGraph) -> list[tuple[str,str]]:\n",
    "    \"\"\"Return list of (ancestor, descendant) conflicts; empty means OK.\"\"\"\n",
    "    S = set(map(str, selected_ids))\n",
    "    viol = []\n",
    "    for u in S:\n",
    "        for v in S:\n",
    "            if u != v and v in nx.descendants(graph, u):\n",
    "                viol.append((u, v))\n",
    "    return viol\n",
    "\n",
    "collapsed_region_ids = collapse_tree_to_n_antichain(G, TARGET_N, root_id=ROOT_ID)\n",
    "collapsed_region_ids = prune_to_antichain(collapsed_region_ids, G)\n",
    "conflicts = verify_antichain(collapsed_region_ids, G)\n",
    "assert not conflicts, f\"Still have ancestor/descendant pairs: {conflicts[:5]}\"\n",
    "\n",
    "# preview\n",
    "preview = [(cid, id_to_name.get(cid, \"\"), id_to_path.get(cid, \"\")) for cid in collapsed_region_ids[:10]]\n",
    "for cid, nm, path in preview:\n",
    "    print(cid, nm, path)\n",
    "\n",
    "def node_depth(sid: str) -> int:\n",
    "    p = id_to_path.get(sid, \"\")\n",
    "    return max(len(str(p).strip(\"/\").split(\"/\")) - 1, 0)\n",
    "\n",
    "# --- 1) verify antichain (no ancestor/descendant pairs) ---\n",
    "conflicts = []\n",
    "S = set(map(str, collapsed_region_ids))\n",
    "for u in S:\n",
    "    # if any selected v is a descendant of u -> conflict\n",
    "    bad = S.intersection(nx.descendants(G, u))\n",
    "    if bad:\n",
    "        conflicts.append((u, list(bad)[:3]))  # keep a few for print\n",
    "assert not conflicts, f\"Antichain violation(s): {conflicts[:5]}\"\n",
    "\n",
    "# --- 2) collapsed_selection.csv ---\n",
    "rows = []\n",
    "for cid in collapsed_region_ids:\n",
    "    sid = str(cid)\n",
    "    nm  = id_to_name.get(sid, \"\")\n",
    "    acr = id_to_acronym.get(sid, \"\")\n",
    "    pth = id_to_path.get(sid, \"\")\n",
    "    d   = node_depth(sid)\n",
    "    # region_id as int if possible\n",
    "    try:\n",
    "        rid = int(sid)\n",
    "    except ValueError:\n",
    "        rid = sid\n",
    "    rows.append({\n",
    "        \"region_id\": rid,\n",
    "        \"acronym\": acr,\n",
    "        \"name\": nm,\n",
    "        \"structure_id_path\": pth,\n",
    "        \"depth\": d\n",
    "    })\n",
    "collapsed_df = pd.DataFrame(rows).sort_values([\"depth\",\"region_id\"])\n",
    "collapsed_df.to_csv(sel_csv, index=False)\n",
    "\n",
    "# --- 3) child_to_collapsed_map.csv + unmapped_regions.csv ---\n",
    "selected_set = set(map(str, collapsed_region_ids))\n",
    "map_rows = []\n",
    "unmapped = []\n",
    "for sid, pth in id_to_path.items():       # sid is already str (we coerced earlier)\n",
    "    parts = [p for p in str(pth).strip(\"/\").split(\"/\") if p]\n",
    "    mapped = next((p for p in reversed(parts) if p in selected_set), None)\n",
    "    if mapped is None:\n",
    "        unmapped.append({\n",
    "            \"child_region_id\": int(sid) if sid.isdigit() else sid,\n",
    "            \"child_name\": id_to_name.get(sid, \"\"),\n",
    "            \"child_acronym\": id_to_acronym.get(sid, \"\"),\n",
    "            \"child_structure_id_path\": pth\n",
    "        })\n",
    "        continue\n",
    "    map_rows.append({\n",
    "        \"child_region_id\": int(sid) if sid.isdigit() else sid,\n",
    "        \"child_name\": id_to_name.get(sid, \"\"),\n",
    "        \"child_acronym\": id_to_acronym.get(sid, \"\"),\n",
    "        \"child_structure_id_path\": pth,\n",
    "        \"collapsed_region_id\": int(mapped) if mapped.isdigit() else mapped,\n",
    "        \"collapsed_name\": id_to_name.get(mapped, \"\"),\n",
    "        \"collapsed_acronym\": id_to_acronym.get(mapped, \"\"),\n",
    "        \"collapsed_structure_id_path\": id_to_path.get(mapped, \"\"),\n",
    "        \"collapsed_depth\": node_depth(mapped)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(map_rows).sort_values([\"collapsed_depth\",\"collapsed_region_id\",\"child_region_id\"]).to_csv(map_csv, index=False)\n",
    "pd.DataFrame(unmapped).to_csv(unmap_csv, index=False)\n",
    "\n",
    "print(f\"[OK] Collapsed selection: {len(collapsed_region_ids)} regions → {sel_csv}\")\n",
    "print(f\"[OK] Child→collapsed map: {len(map_rows)} rows → {map_csv}\")\n",
    "if unmapped:\n",
    "    print(f\"[WARN] Unmapped regions: {len(unmapped)} → {unmap_csv}\")\n",
    "else:\n",
    "    print(\"[OK] All regions mapped to a collapsed parent.\")\n",
    "\n",
    "# === Map child → nearest collapsed parent (by walking up the id_path) ===\n",
    "structure_to_collapsed: dict[str, str] = {}\n",
    "for sid, path in id_to_path.items():\n",
    "    parts = path.strip(\"/\").split(\"/\")\n",
    "    for node in reversed(parts):\n",
    "        if node in collapsed_region_ids:\n",
    "            structure_to_collapsed[str(sid)] = node\n",
    "            break\n",
    "\n",
    "# === Process all mice (each sheet = one mouse), keep L/R separate ===\n",
    "# Expect per-mouse sheets with metadata + optional columns \"L\" and \"R\"\n",
    "df_by_mouse = pd.read_excel(input_excel, sheet_name=None)\n",
    "\n",
    "mean_L: dict[str, dict[str, float]] = defaultdict(dict)  # mouse -> {collapsed_id: mean}\n",
    "std_L:  dict[str, dict[str, float]] = defaultdict(dict)\n",
    "sem_L:  dict[str, dict[str, float]] = defaultdict(dict)\n",
    "\n",
    "mean_R: dict[str, dict[str, float]] = defaultdict(dict)\n",
    "std_R:  dict[str, dict[str, float]] = defaultdict(dict)\n",
    "sem_R:  dict[str, dict[str, float]] = defaultdict(dict)\n",
    "\n",
    "log_rows = []\n",
    "\n",
    "for sheet_name, df_mouse in df_by_mouse.items():\n",
    "    if str(sheet_name).lower() == \"summary\":\n",
    "        continue\n",
    "\n",
    "    # We need structure_id_path. If missing but region_id present, reconstruct from structures.csv\n",
    "    if \"structure_id_path\" not in df_mouse.columns:\n",
    "        if \"region_id\" in df_mouse.columns:\n",
    "            df_mouse[\"structure_id_path\"] = (\n",
    "                df_mouse[\"region_id\"]\n",
    "                .map(lambda x: id_to_path.get(int(x), \"\"))\n",
    "                .astype(str)\n",
    "            )\n",
    "        else:\n",
    "            # If there's only structure_name, you can’t reliably map to ID path — skip this sheet\n",
    "            print(f\"[WARN] Sheet {sheet_name}: no structure_id_path/region_id; skipping.\")\n",
    "            continue\n",
    "\n",
    "    # Detect hemisphere columns. Our earlier exporter named them 'L'/'R'.\n",
    "    has_L = \"L\" in df_mouse.columns\n",
    "    has_R = \"R\" in df_mouse.columns\n",
    "    if not (has_L or has_R):\n",
    "        # Fall back: look for cells_per_mm3_L / cells_per_mm3_R\n",
    "        for candidate in df_mouse.columns:\n",
    "            if candidate.endswith(\"_L\"):\n",
    "                df_mouse[\"L\"] = df_mouse[candidate]\n",
    "                has_L = True\n",
    "            if candidate.endswith(\"_R\"):\n",
    "                df_mouse[\"R\"] = df_mouse[candidate]\n",
    "                has_R = True\n",
    "\n",
    "    if not (has_L or has_R):\n",
    "        print(f\"[WARN] Sheet {sheet_name}: no L/R columns; skipping.\")\n",
    "        continue\n",
    "\n",
    "    mouse_id = str(sheet_name)\n",
    "\n",
    "    # temp assignments per hemisphere: collapsed_id -> list of child values\n",
    "    assign_L: dict[str, list[float]] = defaultdict(list)\n",
    "    assign_R: dict[str, list[float]] = defaultdict(list)\n",
    "\n",
    "    for _, row in df_mouse.iterrows():\n",
    "        path = str(row[\"structure_id_path\"]).strip(\"/\")\n",
    "        if not path:\n",
    "            continue\n",
    "        region_id = path.split(\"/\")[-1]\n",
    "        collapsed_id = structure_to_collapsed.get(region_id)\n",
    "        if not collapsed_id:\n",
    "            continue\n",
    "\n",
    "        if has_L:\n",
    "            vL = row[\"L\"]\n",
    "            if pd.notna(vL):\n",
    "                assign_L[collapsed_id].append(float(vL))\n",
    "                log_rows.append({\n",
    "                    \"mouse\": mouse_id,\n",
    "                    \"hemisphere\": \"L\",\n",
    "                    \"collapsed_region_id\": collapsed_id,\n",
    "                    \"collapsed_region_name\": id_to_name.get(int(collapsed_id), \"\"),\n",
    "                    \"child_region_id\": region_id,\n",
    "                    \"child_region_name\": id_to_name.get(int(region_id), \"\"),\n",
    "                    \"child_structure_id_path\": row[\"structure_id_path\"],\n",
    "                    \"cells_per_mm3_contribution\": float(vL),\n",
    "                })\n",
    "        if has_R:\n",
    "            vR = row[\"R\"]\n",
    "            if pd.notna(vR):\n",
    "                assign_R[collapsed_id].append(float(vR))\n",
    "                log_rows.append({\n",
    "                    \"mouse\": mouse_id,\n",
    "                    \"hemisphere\": \"R\",\n",
    "                    \"collapsed_region_id\": collapsed_id,\n",
    "                    \"collapsed_region_name\": id_to_name.get(int(collapsed_id), \"\"),\n",
    "                    \"child_region_id\": region_id,\n",
    "                    \"child_region_name\": id_to_name.get(int(region_id), \"\"),\n",
    "                    \"child_structure_id_path\": row[\"structure_id_path\"],\n",
    "                    \"cells_per_mm3_contribution\": float(vR),\n",
    "                })\n",
    "\n",
    "    # reduce to mean/std/sem per collapsed region\n",
    "    for cid, vals in assign_L.items():\n",
    "        arr = np.asarray(vals, dtype=float)\n",
    "        m = float(np.mean(arr)) if arr.size else np.nan\n",
    "        s = float(np.std(arr, ddof=1)) if arr.size > 1 else 0.0\n",
    "        e = float(s / np.sqrt(arr.size)) if arr.size > 1 else 0.0\n",
    "        mean_L[mouse_id][cid] = m\n",
    "        std_L[mouse_id][cid]  = s\n",
    "        sem_L[mouse_id][cid]  = e\n",
    "\n",
    "    for cid, vals in assign_R.items():\n",
    "        arr = np.asarray(vals, dtype=float)\n",
    "        m = float(np.mean(arr)) if arr.size else np.nan\n",
    "        s = float(np.std(arr, ddof=1)) if arr.size > 1 else 0.0\n",
    "        e = float(s / np.sqrt(arr.size)) if arr.size > 1 else 0.0\n",
    "        mean_R[mouse_id][cid] = m\n",
    "        std_R[mouse_id][cid]  = s\n",
    "        sem_R[mouse_id][cid]  = e\n",
    "\n",
    "# === Build wide dataframes with meta: columns = <mouse>_L / <mouse>_R ===\n",
    "def build_df_hemis(mean_L, mean_R, label: str):\n",
    "    \"\"\"\n",
    "    mean_L/mean_R: dict mouse -> {collapsed_id: value}\n",
    "    label: thing we're summarizing ('cells_per_mm3')\n",
    "    \"\"\"\n",
    "    cols_dict = {}\n",
    "    all_mice = sorted(set(mean_L.keys()) | set(mean_R.keys()))\n",
    "    for m in all_mice:\n",
    "        if m in mean_L and mean_L[m]:\n",
    "            cols_dict[f\"{m}_L\"] = mean_L[m]\n",
    "        if m in mean_R and mean_R[m]:\n",
    "            cols_dict[f\"{m}_R\"] = mean_R[m]\n",
    "\n",
    "    # rows are collapsed_region_id (as index)\n",
    "    df = pd.DataFrame(cols_dict)\n",
    "    df.index.name = \"collapsed_region_id\"\n",
    "\n",
    "    # attach metadata\n",
    "    df[\"region_id\"] = df.index.astype(int)\n",
    "    df[\"name\"] = df[\"region_id\"].map(lambda x: id_to_name.get(x, \"\"))\n",
    "    df[\"acronym\"] = df[\"region_id\"].map(lambda x: id_to_acronym.get(x, \"\"))\n",
    "    df[\"structure_id_path\"] = df[\"region_id\"].map(lambda x: id_to_path.get(x, \"\"))\n",
    "    df[\"depth\"] = df[\"structure_id_path\"].map(lambda p: len(str(p).strip(\"/\").split(\"/\")))\n",
    "\n",
    "    meta_cols = [\"region_id\", \"name\", \"acronym\", \"structure_id_path\", \"depth\"]\n",
    "    ordered = meta_cols + [c for c in df.columns if c not in meta_cols]\n",
    "    return df[ordered].reset_index(drop=True)\n",
    "\n",
    "df_mean = build_df_hemis(mean_L, mean_R, label=\"cells_per_mm3\")\n",
    "df_std  = build_df_hemis(std_L,  std_R,  label=\"cells_per_mm3\")\n",
    "df_sem  = build_df_hemis(sem_L,  sem_R,  label=\"cells_per_mm3\")\n",
    "\n",
    "# === Export all ===\n",
    "with pd.ExcelWriter(output_excel) as writer:\n",
    "    df_mean.to_excel(writer, sheet_name=\"mean_cells_per_mm3\", index=False)\n",
    "    df_std.to_excel(writer,  sheet_name=\"std_cells_per_mm3\",  index=False)\n",
    "    df_sem.to_excel(writer,  sheet_name=\"sem_cells_per_mm3\",  index=False)\n",
    "\n",
    "print(f\"Collapsed L/R mean/std/sem saved to:\\n{output_excel}\")\n",
    "\n",
    "# Save log\n",
    "pd.DataFrame(log_rows).to_csv(output_log, index=False)\n",
    "print(f\"Detailed density collapsing log saved to:\\n{output_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfa4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DIAG] Category coverage:\n",
      "  - Medulla: 0 nodes under prefix 997/8/343/1065/354/  <<< CHECK THIS PREFIX\n",
      "  - Pons: 0 nodes under prefix 997/8/343/1065/771/  <<< CHECK THIS PREFIX\n",
      "  - Hypothalamus: 0 nodes under prefix 997/8/343/1129/1097/  <<< CHECK THIS PREFIX\n",
      "  - Thalamus: 0 nodes under prefix 997/8/343/1129/549/  <<< CHECK THIS PREFIX\n",
      "  - Midbrain: 0 nodes under prefix 997/8/343/313/  <<< CHECK THIS PREFIX\n",
      "  - Cerebellum: 0 nodes under prefix 997/8/512/  <<< CHECK THIS PREFIX\n",
      "  - Cortical plate: 0 nodes under prefix 997/8/567/688/695/  <<< CHECK THIS PREFIX\n",
      "  - Cortical subplate: 0 nodes under prefix 997/8/567/688/703/  <<< CHECK THIS PREFIX\n",
      "  - Pallidum: 0 nodes under prefix 997/8/567/623/803/  <<< CHECK THIS PREFIX\n",
      "  - Striatum: 0 nodes under prefix 997/8/567/623/477/  <<< CHECK THIS PREFIX\n",
      "\n",
      "[WARN] No nodes selected under current constraints. Retrying allowing category roots...\n",
      "[SELECT] Still selected 0 nodes — check category prefixes in the DIAG output above.\n",
      "[OK] Collapsed matrices written → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsed_matrix.xlsx\n",
      "[OK] Contribution log written → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsing_log.csv (rows=0)\n",
      "[AUDIT] collapsed_selection → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\collapsed_selection.csv (0 rows)\n",
      "[AUDIT] child_to_collapsed_map → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\child_to_collapsed_map.csv (0 rows)\n",
      "[AUDIT] unmapped_regions → Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\unmapped_regions.csv  (count=840)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "structures_path = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\structures.csv\"\n",
    "input_excel     = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\per_mouse_sheets.xlsx\"\n",
    "output_excel    = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsed_matrix.xlsx\"\n",
    "output_log      = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\\den_collapsing_log.csv\"\n",
    "audit_dir       = r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\PE_mapping\\processed_data\"\n",
    "\n",
    "TARGET_N        = 160\n",
    "ROOT_ID         = \"997\"   # Allen mouse root\n",
    "BAN_IDS         = {\"8\"}   # ban shallow hub: \"Basic cell groups and regions\"\n",
    "\n",
    "# Category roots: nothing ABOVE these IDs will ever be selected\n",
    "category_map = {\n",
    "    \"Medulla\": \"/997/8/343/1065/354/\",\n",
    "    \"Pons\": \"/997/8/343/1065/771/\",\n",
    "    \"Hypothalamus\": \"/997/8/343/1129/1097/\",\n",
    "    \"Thalamus\": \"/997/8/343/1129/549/\",\n",
    "    \"Midbrain\": \"/997/8/343/313/\",\n",
    "    \"Cerebellum\": \"/997/8/512/\",\n",
    "    \"Cortical plate\": \"/997/8/567/688/695/\",\n",
    "    \"Cortical subplate\": \"/997/8/567/688/703/\",\n",
    "    \"Pallidum\": \"/997/8/567/623/803/\",\n",
    "    \"Striatum\": \"/997/8/567/623/477/\",\n",
    "}\n",
    "# Selection behavior inside categories\n",
    "MIN_OFFSET_BELOW_CATEGORY = 1     # require at least 1 level below the category root\n",
    "ALLOW_CATEGORY_ROOTS      = False # forbid selecting the category root itself initially\n",
    "RELAX_DEPTH_IF_N_SHORT    = True  # carefully relax toward the root if needed\n",
    "# ==================================================\n",
    "def canon(p) -> str:\n",
    "    \"\"\"Ensure leading and trailing slash: '/.../'.\"\"\"\n",
    "    p = \"\" if p is None else str(p)\n",
    "    return \"/\" + p.strip(\"/\") + \"/\"\n",
    "\n",
    "\n",
    "# ------------------ Structures & Graph ------------------\n",
    "def load_structures(structures_csv: str):\n",
    "    S = pd.read_csv(structures_path)\n",
    "    S[\"id\"] = S[\"id\"].astype(str)\n",
    "    S[\"structure_id_path\"] = S[\"structure_id_path\"].apply(canon)\n",
    "\n",
    "    id_to_acronym = S.set_index(\"id\")[\"acronym\"].astype(str).to_dict()\n",
    "    id_to_path    = S.set_index(\"id\")[\"structure_id_path\"].to_dict()  # canonical now\n",
    "    id_to_name    = S.set_index(\"id\")[\"name\"].astype(str).to_dict()\n",
    "    return S, id_to_name, id_to_path, id_to_acronym\n",
    "\n",
    "def build_graph_from_paths(S: pd.DataFrame) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in S.iterrows():\n",
    "        # row[\"structure_id_path\"] is already canonical '/.../'\n",
    "        parts = row[\"structure_id_path\"].strip(\"/\").split(\"/\")\n",
    "        for i in range(len(parts) - 1):\n",
    "            G.add_edge(parts[i], parts[i + 1])  # IDs are strings\n",
    "    return G\n",
    "\n",
    "\n",
    "def node_depth_from_path(sid: str, id_to_path: dict) -> int:\n",
    "    p = id_to_path.get(sid, \"\")\n",
    "    return max(len(str(p).strip(\"/\").split(\"/\")) - 1, 0)\n",
    "\n",
    "# ------------------ Diagnostics ------------------\n",
    "def diagnose_categories(id_to_path: dict, category_map: dict):\n",
    "    print(\"\\n[DIAG] Category coverage:\")\n",
    "    for name, pref in category_map.items():\n",
    "        pref_s = canon(pref)                     # <-- canonicalize the prefix\n",
    "        depths, count = [], 0\n",
    "        for sid, pth in id_to_path.items():      # pth already canonical\n",
    "            if pth.startswith(pref_s):\n",
    "                count += 1\n",
    "                depths.append(len(pth.strip(\"/\").split(\"/\")) - 1)\n",
    "        if count == 0:\n",
    "            print(f\"  - {name}: 0 nodes under prefix {pref_s}  <<< CHECK THIS PREFIX\")\n",
    "        else:\n",
    "            print(f\"  - {name}: {count} nodes; depth range {min(depths)}–{max(depths)} (prefix {pref_s})\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# --------- Category-bounded antichain selection ----------\n",
    "def select_collapsed_antichain_within_categories(\n",
    "    G,\n",
    "    id_to_path: dict,\n",
    "    category_map: dict,\n",
    "    target_n: int,\n",
    "    root_id: str = \"997\",\n",
    "    *,\n",
    "    min_offset: int = 1,\n",
    "    allow_category_roots: bool = False,\n",
    "    ban_ids: set | None = None,\n",
    "    relax: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Pick up to target_n region IDs as an antichain (no ancestor/descendant pairs),\n",
    "    constrained to lie INSIDE the provided category subtrees (never above them).\n",
    "\n",
    "    Ranking: deeper nodes first, then root→leaf path frequency.\n",
    "    \"\"\"\n",
    "    ban_ids = set(map(str, ban_ids or set()))\n",
    "    root_id = str(root_id)\n",
    "\n",
    "    # Canonicalize category prefixes (ensure '/.../')\n",
    "    cat_prefixes = [canon(p) for p in category_map.values()]\n",
    "    cat_roots    = {pref.strip(\"/\").split(\"/\")[-1] for pref in cat_prefixes}\n",
    "\n",
    "    def depth(sid: str) -> int:\n",
    "        return node_depth_from_path(sid, id_to_path)\n",
    "\n",
    "    def in_categories(sid: str):\n",
    "        \"\"\"Return (inside_any_category, min_required_depth) for sid, using canonical paths.\"\"\"\n",
    "        p = id_to_path.get(sid, \"\")  # already canonical from load_structures\n",
    "        if not p:\n",
    "            return False, 10**9\n",
    "        best_root_depth = -1\n",
    "        for pref in cat_prefixes:\n",
    "            if p.startswith(pref):\n",
    "                d = len(pref.strip(\"/\").split(\"/\")) - 1\n",
    "                if d > best_root_depth:\n",
    "                    best_root_depth = d\n",
    "        if best_root_depth < 0:\n",
    "            return False, 10**9\n",
    "        return True, best_root_depth + min_offset\n",
    "\n",
    "\n",
    "    def depth(sid: str) -> int:\n",
    "        return node_depth_from_path(sid, id_to_path)\n",
    "\n",
    "    # Root->leaf path frequency counts (strings everywhere)\n",
    "    leaves = [v for v in G.nodes if G.out_degree(v) == 0 and nx.has_path(G, root_id, v)]\n",
    "    counts = Counter()\n",
    "    for leaf in leaves:\n",
    "        for v in nx.shortest_path(G, root_id, leaf):\n",
    "            counts[v] += 1\n",
    "\n",
    "    # Build candidate list under current offset\n",
    "    def candidates_for_offset(offset: int):\n",
    "        C = []\n",
    "        for v in counts.keys():\n",
    "            if v == root_id or v in ban_ids:\n",
    "                continue\n",
    "            inside, min_depth_req = in_categories(v)\n",
    "            if not inside:\n",
    "                continue\n",
    "            if (not allow_category_roots) and (v in cat_roots):\n",
    "                continue\n",
    "            # relax toward roots by reducing the min depth requirement\n",
    "            min_depth_adj = min_depth_req - (min_offset - offset)\n",
    "            if depth(v) >= min_depth_adj:\n",
    "                C.append(v)\n",
    "        # Rank: deeper first, then frequency, then ID for reproducibility\n",
    "        C.sort(key=lambda x: (depth(x), counts[x], x), reverse=True)\n",
    "        return C\n",
    "\n",
    "    def greedy_antichain_pick(cands):\n",
    "        selected, selset = [], set()\n",
    "        for v in cands:\n",
    "            # forbid ancestor/descendant conflicts\n",
    "            if any((v in nx.descendants(G, s)) or (s in nx.descendants(G, v)) for s in selset):\n",
    "                continue\n",
    "            selset.add(v)\n",
    "            selected.append(v)\n",
    "            if len(selected) >= target_n:\n",
    "                break\n",
    "        return selected\n",
    "\n",
    "    # Pass 1: strict (min_offset)\n",
    "    selected = greedy_antichain_pick(candidates_for_offset(min_offset))\n",
    "\n",
    "    # Pass 2: optional relax within categories\n",
    "    if relax and len(selected) < target_n:\n",
    "        for off in range(min_offset - 1, -1, -1):\n",
    "            cands = [v for v in candidates_for_offset(off) if v not in selected]\n",
    "            if not cands:\n",
    "                continue\n",
    "            curr = list(selected); selset = set(curr)\n",
    "            for v in cands:\n",
    "                if any((v in nx.descendants(G, s)) or (s in nx.descendants(G, v)) for s in selset):\n",
    "                    continue\n",
    "                selset.add(v); curr.append(v)\n",
    "                if len(curr) >= target_n:\n",
    "                    break\n",
    "            selected = curr\n",
    "            if len(selected) >= target_n:\n",
    "                break\n",
    "\n",
    "    return selected\n",
    "\n",
    "# -------------- Mapping & Aggregation --------------\n",
    "def map_child_to_collapsed(id_to_path: dict, selected_ids: list[str]) -> dict:\n",
    "    \"\"\"Map every atlas region id -> nearest selected ancestor (if any).\"\"\"\n",
    "    selected = set(selected_ids)\n",
    "    mapping = {}\n",
    "    for sid, pth in id_to_path.items():\n",
    "        parts = [p for p in str(pth).strip(\"/\").split(\"/\") if p]\n",
    "        mapped = next((p for p in reversed(parts) if p in selected), None)\n",
    "        if mapped:\n",
    "            mapping[sid] = mapped\n",
    "    return mapping\n",
    "\n",
    "def aggregate_per_mouse_hemi(input_excel: str,\n",
    "                             structure_to_collapsed: dict,\n",
    "                             id_to_name: dict,\n",
    "                             id_to_path: dict):\n",
    "    \"\"\"\n",
    "    Read per_mouse_sheets.xlsx and aggregate mean/std/sem across child regions, per mouse & hemisphere.\n",
    "    Accepts L/R or any *_L / *_R columns.\n",
    "    Reconstructs structure_id_path from region_id if needed.\n",
    "    \"\"\"\n",
    "    df_by_mouse = pd.read_excel(input_excel, sheet_name=None)\n",
    "    mean_L, std_L, sem_L = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "    mean_R, std_R, sem_R = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "    log_rows = []\n",
    "\n",
    "    for sheet_name, df_mouse in df_by_mouse.items():\n",
    "        if str(sheet_name).lower() == \"summary\":\n",
    "            continue\n",
    "\n",
    "        # Ensure structure_id_path exists (reconstruct from region_id if available)\n",
    "        if \"structure_id_path\" not in df_mouse.columns:\n",
    "            if \"region_id\" in df_mouse.columns:\n",
    "                df_mouse = df_mouse.copy()\n",
    "                df_mouse[\"structure_id_path\"] = df_mouse[\"region_id\"].astype(str).map(id_to_path).fillna(\"\")\n",
    "            else:\n",
    "                print(f\"[WARN] {sheet_name}: missing structure_id_path; skipping.\")\n",
    "                continue\n",
    "\n",
    "        # Detect hemisphere columns (L/R). Fallback: any *_L / *_R\n",
    "        has_L = \"L\" in df_mouse.columns\n",
    "        has_R = \"R\" in df_mouse.columns\n",
    "        if not (has_L or has_R):\n",
    "            for c in df_mouse.columns:\n",
    "                if c.endswith(\"_L\") and not has_L:\n",
    "                    df_mouse[\"L\"] = df_mouse[c]; has_L = True\n",
    "                if c.endswith(\"_R\") and not has_R:\n",
    "                    df_mouse[\"R\"] = df_mouse[c]; has_R = True\n",
    "        if not (has_L or has_R):\n",
    "            print(f\"[WARN] {sheet_name}: no L/R columns; skipping.\")\n",
    "            continue\n",
    "\n",
    "        mouse_id = str(sheet_name)\n",
    "        assign_L, assign_R = defaultdict(list), defaultdict(list)\n",
    "\n",
    "        for _, row in df_mouse.iterrows():\n",
    "            path = str(row[\"structure_id_path\"]).strip(\"/\")\n",
    "            if not path:\n",
    "                continue\n",
    "            region_id = path.split(\"/\")[-1]  # leaf id as string\n",
    "            cid = structure_to_collapsed.get(region_id)\n",
    "            if not cid:\n",
    "                continue\n",
    "\n",
    "            if has_L and pd.notna(row[\"L\"]):\n",
    "                vL = float(row[\"L\"])\n",
    "                assign_L[cid].append(vL)\n",
    "                log_rows.append({\n",
    "                    \"mouse\": mouse_id, \"hemisphere\": \"L\",\n",
    "                    \"collapsed_region_id\": cid,\n",
    "                    \"collapsed_region_name\": id_to_name.get(cid, \"\"),\n",
    "                    \"child_region_id\": region_id,\n",
    "                    \"child_structure_id_path\": row[\"structure_id_path\"],\n",
    "                    \"cells_per_mm3_contribution\": vL\n",
    "                })\n",
    "            if has_R and pd.notna(row[\"R\"]):\n",
    "                vR = float(row[\"R\"])\n",
    "                assign_R[cid].append(vR)\n",
    "                log_rows.append({\n",
    "                    \"mouse\": mouse_id, \"hemisphere\": \"R\",\n",
    "                    \"collapsed_region_id\": cid,\n",
    "                    \"collapsed_region_name\": id_to_name.get(cid, \"\"),\n",
    "                    \"child_region_id\": region_id,\n",
    "                    \"child_structure_id_path\": row[\"structure_id_path\"],\n",
    "                    \"cells_per_mm3_contribution\": vR\n",
    "                })\n",
    "\n",
    "        # reduce to mean/std/sem per collapsed region\n",
    "        for cid, vals in assign_L.items():\n",
    "            arr = np.asarray(vals, float)\n",
    "            m = float(np.mean(arr))\n",
    "            s = float(np.std(arr, ddof=1)) if arr.size > 1 else 0.0\n",
    "            e = float(s / np.sqrt(arr.size)) if arr.size > 1 else 0.0\n",
    "            mean_L[mouse_id][cid] = m; std_L[mouse_id][cid] = s; sem_L[mouse_id][cid] = e\n",
    "\n",
    "        for cid, vals in assign_R.items():\n",
    "            arr = np.asarray(vals, float)\n",
    "            m = float(np.mean(arr))\n",
    "            s = float(np.std(arr, ddof=1)) if arr.size > 1 else 0.0\n",
    "            e = float(s / np.sqrt(arr.size)) if arr.size > 1 else 0.0\n",
    "            mean_R[mouse_id][cid] = m; std_R[mouse_id][cid] = s; sem_R[mouse_id][cid] = e\n",
    "\n",
    "    return (mean_L, std_L, sem_L), (mean_R, std_R, sem_R), log_rows\n",
    "\n",
    "def build_wide_with_meta(mean_L, mean_R, id_to_name, id_to_path, id_to_acronym):\n",
    "    \"\"\"Build a wide table: meta columns + one column per mouse hemisphere (<mouse>_L / <mouse>_R).\"\"\"\n",
    "    cols = {}\n",
    "    all_mice = sorted(set(mean_L.keys()) | set(mean_R.keys()))\n",
    "    for m in all_mice:\n",
    "        if m in mean_L and mean_L[m]:\n",
    "            cols[f\"{m}_L\"] = mean_L[m]   # dict: collapsed_id -> value\n",
    "        if m in mean_R and mean_R[m]:\n",
    "            cols[f\"{m}_R\"] = mean_R[m]\n",
    "\n",
    "    df = pd.DataFrame(cols)\n",
    "    df.index.name = \"collapsed_region_id\"\n",
    "\n",
    "    # attach metadata\n",
    "    df[\"region_id\"] = df.index.astype(str)  # keep as string key for maps\n",
    "    df[\"acronym\"] = df[\"region_id\"].map(lambda x: id_to_acronym.get(x, \"\"))\n",
    "    df[\"name\"] = df[\"region_id\"].map(lambda x: id_to_name.get(x, \"\"))\n",
    "    df[\"structure_id_path\"] = df[\"region_id\"].map(lambda x: id_to_path.get(x, \"\"))\n",
    "    df[\"depth\"] = df[\"structure_id_path\"].map(lambda p: len(str(p).strip(\"/\").split(\"/\")))\n",
    "\n",
    "    # if you prefer integer region_id in output, coerce safely\n",
    "    def to_int_maybe(x):\n",
    "        try: return int(x)\n",
    "        except: return x\n",
    "    df[\"region_id\"] = df[\"region_id\"].map(to_int_maybe)\n",
    "\n",
    "    meta_cols = [\"region_id\", \"acronym\", \"name\", \"structure_id_path\", \"depth\"]\n",
    "    ordered = meta_cols + [c for c in df.columns if c not in meta_cols]\n",
    "    return df[ordered].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# -------------------- Audit Exports (safe) --------------------\n",
    "def export_audit_files(collapsed_region_ids, id_to_name, id_to_acronym, id_to_path,\n",
    "                       structure_to_collapsed, audit_dir: str):\n",
    "    \"\"\"Write: collapsed_selection.csv, child_to_collapsed_map.csv, unmapped_regions.csv. Safe if empty selection.\"\"\"\n",
    "    out_dir = Path(audit_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sel_csv  = out_dir / \"collapsed_selection.csv\"\n",
    "    map_csv  = out_dir / \"child_to_collapsed_map.csv\"\n",
    "    unmap_csv= out_dir / \"unmapped_regions.csv\"\n",
    "\n",
    "    # 1) collapsed_selection.csv\n",
    "    rows = []\n",
    "    for cid in collapsed_region_ids:\n",
    "        sid = str(cid)\n",
    "        rows.append({\n",
    "            \"region_id\": int(sid) if sid.isdigit() else sid,\n",
    "            \"acronym\":   id_to_acronym.get(sid, \"\"),\n",
    "            \"name\":      id_to_name.get(sid, \"\"),\n",
    "            \"structure_id_path\": id_to_path.get(sid, \"\"),\n",
    "            \"depth\":     node_depth_from_path(sid, id_to_path)\n",
    "        })\n",
    "    sel_df = pd.DataFrame(rows)\n",
    "    if not sel_df.empty:\n",
    "        sel_df.sort_values([\"depth\",\"region_id\"]).to_csv(sel_csv, index=False)\n",
    "    else:\n",
    "        # write headers so downstream tools don't choke\n",
    "        pd.DataFrame(columns=[\"region_id\",\"acronym\",\"name\",\"structure_id_path\",\"depth\"]).to_csv(sel_csv, index=False)\n",
    "\n",
    "    # 2) child_to_collapsed_map.csv\n",
    "    map_rows, unmapped = [], []\n",
    "    selected_set = set(map(str, collapsed_region_ids))\n",
    "    for sid, pth in id_to_path.items():\n",
    "        parts = [p for p in str(pth).strip(\"/\").split(\"/\") if p]\n",
    "        mapped = next((p for p in reversed(parts) if p in selected_set), None)\n",
    "        if mapped is None:\n",
    "            unmapped.append({\n",
    "                \"child_region_id\": int(sid) if sid.isdigit() else sid,\n",
    "                \"child_name\": id_to_name.get(sid, \"\"),\n",
    "                \"child_acronym\": id_to_acronym.get(sid, \"\"),\n",
    "                \"child_structure_id_path\": pth\n",
    "            })\n",
    "        else:\n",
    "            map_rows.append({\n",
    "                \"child_region_id\": int(sid) if sid.isdigit() else sid,\n",
    "                \"child_name\": id_to_name.get(sid, \"\"),\n",
    "                \"child_acronym\": id_to_acronym.get(sid, \"\"),\n",
    "                \"child_structure_id_path\": pth,\n",
    "                \"collapsed_region_id\": int(mapped) if mapped.isdigit() else mapped,\n",
    "                \"collapsed_name\": id_to_name.get(mapped, \"\"),\n",
    "                \"collapsed_acronym\": id_to_acronym.get(mapped, \"\"),\n",
    "                \"collapsed_structure_id_path\": id_to_path.get(mapped, \"\"),\n",
    "                \"collapsed_depth\": node_depth_from_path(mapped, id_to_path)\n",
    "            })\n",
    "    pd.DataFrame(map_rows).to_csv(map_csv, index=False)\n",
    "    pd.DataFrame(unmapped).to_csv(unmap_csv, index=False)\n",
    "\n",
    "    print(f\"[AUDIT] collapsed_selection → {sel_csv} ({len(sel_df)} rows)\")\n",
    "    print(f\"[AUDIT] child_to_collapsed_map → {map_csv} ({len(map_rows)} rows)\")\n",
    "    if unmapped:\n",
    "        print(f\"[AUDIT] unmapped_regions → {unmap_csv}  (count={len(unmapped)})\")\n",
    "    else:\n",
    "        print(\"[AUDIT] All regions mapped.\")\n",
    "\n",
    "\n",
    "# ------------------------- MAIN -------------------------\n",
    "def main():\n",
    "    # 1) Load structures & build graph\n",
    "    S, id_to_name, id_to_path, id_to_acronym = load_structures(structures_path)\n",
    "    G = build_graph_from_paths(S)\n",
    "    if ROOT_ID not in G:\n",
    "        raise ValueError(f\"Root id {ROOT_ID} not in graph.\")\n",
    "\n",
    "    # Diagnostics: ensure your prefixes actually match nodes\n",
    "    diagnose_categories(id_to_path, category_map)\n",
    "\n",
    "    # 2) Select collapsed regions **within categories** (never above them)\n",
    "    collapsed_region_ids = select_collapsed_antichain_within_categories(\n",
    "        G,\n",
    "        id_to_path=id_to_path,\n",
    "        category_map=category_map,\n",
    "        target_n=TARGET_N,\n",
    "        root_id=ROOT_ID,\n",
    "        min_offset=MIN_OFFSET_BELOW_CATEGORY,\n",
    "        allow_category_roots=ALLOW_CATEGORY_ROOTS,\n",
    "        ban_ids=BAN_IDS,\n",
    "        relax=RELAX_DEPTH_IF_N_SHORT,\n",
    "    )\n",
    "\n",
    "    # Fallback if empty or too small: allow category roots\n",
    "    if len(collapsed_region_ids) == 0:\n",
    "        print(\"[WARN] No nodes selected under current constraints. Retrying allowing category roots...\")\n",
    "        collapsed_region_ids = select_collapsed_antichain_within_categories(\n",
    "            G, id_to_path, category_map, TARGET_N, root_id=ROOT_ID,\n",
    "            min_offset=0, allow_category_roots=True, ban_ids=BAN_IDS, relax=True\n",
    "        )\n",
    "\n",
    "    depths = [node_depth_from_path(s, id_to_path) for s in collapsed_region_ids] if collapsed_region_ids else []\n",
    "    if collapsed_region_ids:\n",
    "        print(f\"[SELECT] Selected {len(collapsed_region_ids)} collapsed nodes within categories. \"\n",
    "              f\"Depth range: {min(depths)}–{max(depths)}\")\n",
    "    else:\n",
    "        print(\"[SELECT] Still selected 0 nodes — check category prefixes in the DIAG output above.\")\n",
    "\n",
    "    # 3) Map every atlas region to its nearest selected ancestor (will be empty map if selection empty)\n",
    "    structure_to_collapsed = map_child_to_collapsed(id_to_path, collapsed_region_ids)\n",
    "\n",
    "    # 4) Aggregate per mouse & hemisphere (cells_per_mm3)\n",
    "    (mean_L, std_L, sem_L), (mean_R, std_R, sem_R), log_rows = aggregate_per_mouse_hemi(\n",
    "        input_excel, structure_to_collapsed, id_to_name, id_to_path\n",
    "    )\n",
    "\n",
    "    # 5) Build wide tables with metadata\n",
    "    if collapsed_region_ids:\n",
    "        df_mean = build_wide_with_meta(mean_L, mean_R, id_to_name, id_to_path, id_to_acronym)\n",
    "        df_std  = build_wide_with_meta(std_L,  std_R,  id_to_name, id_to_path, id_to_acronym)\n",
    "        df_sem  = build_wide_with_meta(sem_L,  sem_R,  id_to_name, id_to_path, id_to_acronym)\n",
    "    else:\n",
    "        # empty frames with just meta headers\n",
    "        df_mean = pd.DataFrame(columns=[\"region_id\",\"acronym\",\"name\",\"structure_id_path\",\"depth\"])\n",
    "        df_std  = df_mean.copy(); df_sem = df_mean.copy()\n",
    "\n",
    "    # 6) Save outputs\n",
    "    with pd.ExcelWriter(output_excel) as writer:\n",
    "        df_mean.to_excel(writer, sheet_name=\"mean_cells_per_mm3\", index=False)\n",
    "        df_std.to_excel(writer,  sheet_name=\"std_cells_per_mm3\",  index=False)\n",
    "        df_sem.to_excel(writer,  sheet_name=\"sem_cells_per_mm3\",  index=False)\n",
    "    print(f\"[OK] Collapsed matrices written → {output_excel}\")\n",
    "\n",
    "    pd.DataFrame(log_rows).to_csv(output_log, index=False)\n",
    "    print(f\"[OK] Contribution log written → {output_log} (rows={len(log_rows)})\")\n",
    "\n",
    "    # 7) Audit CSVs (safe if empty)\n",
    "    export_audit_files(collapsed_region_ids, id_to_name, id_to_acronym, id_to_path,\n",
    "                       structure_to_collapsed, audit_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "napari-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
