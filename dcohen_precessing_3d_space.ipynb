{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from brainglobe_utils.image.heatmap import heatmap_from_points\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m density_map\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Load a reference atlas from any subject (assuming all atlases have the same shape)\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m reference_atlas \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup1_subjects\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     70\u001b[0m group1_density_map \u001b[38;5;241m=\u001b[39m create_density_map(group1_densities, reference_atlas)\n\u001b[0;32m     71\u001b[0m group2_density_map \u001b[38;5;241m=\u001b[39m create_density_map(group2_densities, reference_atlas)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\brainglobe-test\\Lib\\site-packages\\numpy\\lib\\npyio.py:462\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[1;32m--> 462\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load file containing pickled data \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen allow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(fid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_kwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "allow_pickle=True\n",
    "\n",
    "# Define paths to subjects (Each subject has its own atlas.points and atlas_regions.npy)\n",
    "\n",
    "group1_subjects = [\n",
    "    {\"points\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\026\\brainreg_gr_4\\points\\atlas.points\"),\n",
    "    \"regions\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\026\\brainreg_gr_4\\registration\\registered_atlas.tiff\")},\n",
    "    {\"points\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\833\\brainreg_gr_3\\points\\atlas.points\"),\n",
    "    \"regions\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\833\\brainreg_gr_3\\registration\\registered_atlas.tiff\")},\n",
    "    {\"points\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\987\\brainreg_1\\points\\atlas.points\"),\n",
    "    \"regions\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\987\\brainreg_1\\registration\\registered_atlas.tiff\")},\n",
    "    {\"points\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos61\\brainreg_trained_trained_74model_changedparam_4\\points\\atlas.points\"),\n",
    "    \"regions\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos61\\brainreg_trained_trained_74model_changedparam_4\\registration\\registered_atlas.tiff\")},\n",
    "    {\"points\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos74\\brainreg_trained_BB_10\\points\\atlas.points\"),\n",
    "     \"regions\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos74\\brainreg_trained_BB_10\\registration\\registered_atlas.tiff\")}\n",
    "]\n",
    "\n",
    "group2_subjects = [\n",
    "    {\"points\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\834\\brainreg_gr_mixed_4\\points\\atlas.points\"),\n",
    "    \"regions\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\834\\brainreg_gr_mixed_4\\registration\\registered_atlas.tiff\")},\n",
    "    {\"points\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\838\\brainreg_gr_4\\points\\atlas.points\"),\n",
    "    \"regions\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\838\\brainreg_gr_4\\registration\\registered_atlas.tiff\")},\n",
    "    {\"points\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\981\\brainreg_3\\points\\atlas.points\"),\n",
    "    \"regions\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\981\\brainreg_3\\registration\\registered_atlas.tiff\")},\n",
    "    {\"points\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\cFos60\\brainreg_trained_after74_6\\points\\atlas.points\"),\n",
    "    \"regions\": Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\cFos60\\brainreg_trained_after74_6\\registration\\registered_atlas.tiff\")}\n",
    "]\n",
    "\n",
    "# Function to compute mean density per brain region for a list of subjects\n",
    "def compute_density(subjects):\n",
    "    region_density = {}\n",
    "\n",
    "    for subject in subjects:\n",
    "        # Load subject-specific atlas and cell points\n",
    "        points = pd.read_hdf(subject[\"points\"], key=\"df\").values\n",
    "        atlas_labels = np.load(subject[\"regions\"])  # Load subject-specific region mapping\n",
    "\n",
    "        unique_regions = np.unique(atlas_labels)  # Get all brain regions in this subject's atlas\n",
    "\n",
    "        # Compute histogram (cell count per voxel)\n",
    "        heatmap, _ = np.histogramdd(points, bins=atlas_labels.shape)\n",
    "\n",
    "        # Compute density per region\n",
    "        for region in unique_regions:\n",
    "            region_mask = atlas_labels == region\n",
    "            mean_density = heatmap[region_mask].sum() / region_mask.sum()  # Normalize by voxel count\n",
    "\n",
    "            if region not in region_density:\n",
    "                region_density[region] = []\n",
    "            region_density[region].append(mean_density)\n",
    "\n",
    "    # Compute the mean density across all subjects in the group\n",
    "    mean_densities = {region: np.mean(region_density[region]) for region in region_density}\n",
    "    return mean_densities\n",
    "\n",
    "# Compute mean densities for each group (makes background bright?)\n",
    "group1_densities = compute_density(group1_subjects)\n",
    "group2_densities = compute_density(group2_subjects)\n",
    "\n",
    "# Create brain-wide density maps\n",
    "def create_density_map(region_densities, reference_atlas):\n",
    "    density_map = np.zeros_like(reference_atlas, dtype=float)\n",
    "    for region, value in region_densities.items():\n",
    "        density_map[reference_atlas == region] = value\n",
    "    return density_map\n",
    "\n",
    "# Load a reference atlas from any subject (assuming all atlases have the same shape?)\n",
    "reference_atlas = np.load(group1_subjects[0][\"regions\"])  \n",
    "\n",
    "group1_density_map = create_density_map(group1_densities, reference_atlas)\n",
    "group2_density_map = create_density_map(group2_densities, reference_atlas)\n",
    "\n",
    "# Apply Gaussian smoothing\n",
    "smoothed_group1 = gaussian_filter(group1_density_map, sigma=5)\n",
    "smoothed_group2 = gaussian_filter(group2_density_map, sigma=5)\n",
    "\n",
    "# Generate heatmaps for each group\n",
    "heatmap_from_points(\n",
    "    group1_density_map, \n",
    "    image_resolution=25,\n",
    "    image_shape=reference_atlas.shape,\n",
    "    output_filename=\"group1_density_heatmap.tiff\",\n",
    "    smoothing=50\n",
    ")\n",
    "\n",
    "heatmap_from_points(\n",
    "    group2_density_map, \n",
    "    image_resolution=25,\n",
    "    image_shape=reference_atlas.shape,\n",
    "    output_filename=\"group2_density_heatmap.tiff\",\n",
    "    smoothing=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from brainglobe_utils.image.heatmap import heatmap_from_points\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################################\n",
    "# 1. Define Subjects (each subject has a path to 'atlas.points')\n",
    "##############################################################################\n",
    "group1_points_files = [\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\026\\brainreg_gr_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\833\\brainreg_gr_3\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\987\\brainreg_1\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos61\\brainreg_trained_trained_74model_changedparam_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos74\\brainreg_trained_BB_10\\points\\atlas.points\"),\n",
    "]\n",
    "\n",
    "group1_atlas_files = [\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\026\\brainreg_gr_4\\registration\\registered_atlas.tiff\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\833\\brainreg_gr_3\\registration\\registered_atlas.tiff\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\987\\brainreg_1\\registration\\registered_atlas.tiff\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos61\\brainreg_trained_trained_74model_changedparam_4\\registration\\registered_atlas.tiff\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos74\\brainreg_trained_BB_10\\registration\\registered_atlas.tiff\"),\n",
    "]\n",
    "\n",
    "group2_points_files = [\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\834\\brainreg_gr_mixed_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\838\\brainreg_gr_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\981\\brainreg_3\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\cFos60\\brainreg_trained_after74_6\\points\\atlas.points\")\n",
    "]\n",
    "\n",
    "group2_atlas_files = [\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\834\\brainreg_gr_mixed_4\\registration\\registered_atlas.tiff\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\838\\brainreg_gr_4\\registration\\registered_atlas.tiff\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\981\\brainreg_3\\registration\\registered_atlas.tiff\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\cFos60\\brainreg_trained_after74_6\\registration\\registered_atlas.tiff\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################################\n",
    "# 1. Define Subjects (each subject has a path to 'atlas.points')\n",
    "##############################################################################\n",
    "group1_subjects = [\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\026\\brainreg_gr_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\833\\brainreg_gr_3\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\987\\brainreg_1\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos61\\brainreg_trained_trained_74model_changedparam_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos74\\brainreg_trained_BB_10\\points\\atlas.points\"),\n",
    "]\n",
    "\n",
    "\n",
    "group2_subjects = [\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\834\\brainreg_gr_mixed_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\838\\brainreg_gr_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\981\\brainreg_3\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\cFos60\\brainreg_trained_after74_6\\points\\atlas.points\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 2. Determine the Atlas Shape (bins) & Load Sample Points\n",
    "##############################################################################\n",
    "atlas_shape = (1320, 903, 1173)  # Adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 3. Create Helper Functions\n",
    "##############################################################################\n",
    "\n",
    "def load_points(points_file):\n",
    "    \"\"\"Load cell coordinates (Nx3) from an HDF5 atlas.points file.\"\"\"\n",
    "    df = pd.read_hdf(points_file, key=\"df\")\n",
    "    return df.values  # Nx3 array\n",
    "\n",
    "def subject_hist(points_file, atlas_mask, atlas_shape):\n",
    "    \"\"\"\n",
    "    Convert a subject’s Nx3 points into a 3D masked histogram.\n",
    "    Non-brain regions (mask=0) will be set to NaN.\n",
    "    \"\"\"\n",
    "    coords = load_points(points_file)\n",
    "    x_bins = np.arange(0, atlas_shape[0] + 1)\n",
    "    y_bins = np.arange(0, atlas_shape[1] + 1)\n",
    "    z_bins = np.arange(0, atlas_shape[2] + 1)\n",
    "    \n",
    "    hist, _ = np.histogramdd(coords, bins=[x_bins, y_bins, z_bins])\n",
    "    \n",
    "    # Apply the atlas mask (set non-brain regions to NaN)\n",
    "    masked_hist = hist * atlas_mask\n",
    "    masked_hist[atlas_mask == 0] = np.nan\n",
    "    \n",
    "    return masked_hist\n",
    "\n",
    "def load_group_voxels(points_files, atlas_files, atlas_shape):\n",
    "    \"\"\"\n",
    "    Load histograms for all subjects in a group, applying the specific atlas mask to each.\n",
    "    \"\"\"\n",
    "    all_hists = []\n",
    "    for points_file, atlas_mask_file in zip(points_files, atlas_files):\n",
    "        \n",
    "        # Load the specific atlas mask\n",
    "        atlas_mask = tifffile.imread(str(atlas_mask_file))\n",
    "        atlas_mask = (atlas_mask > 0).astype(np.uint8)  # Ensure binary mask\n",
    "        \n",
    "        # Rescale atlas mask if needed\n",
    "        if atlas_mask.shape != atlas_shape:\n",
    "            print(f\"Rescaling atlas mask from {atlas_mask.shape} to {atlas_shape}\")\n",
    "            scaling_factors = np.array(atlas_shape) / np.array(atlas_mask.shape)\n",
    "            atlas_mask = zoom(atlas_mask, scaling_factors, order=0)  # Nearest-neighbor\n",
    "        \n",
    "        # Load and mask the subject's cell data\n",
    "        masked_hist = subject_hist(points_file, atlas_mask, atlas_shape)\n",
    "        all_hists.append(masked_hist)\n",
    "    \n",
    "    return np.array(all_hists)  # shape: (num_subjects, X, Y, Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaling atlas mask from (1400, 838, 1254) to (1399, 903, 1173)\n",
      "Rescaling atlas mask from (1400, 827, 1204) to (1399, 903, 1173)\n",
      "Rescaling atlas mask from (1412, 1035, 1203) to (1399, 903, 1173)\n",
      "Rescaling atlas mask from (1288, 903, 1173) to (1399, 903, 1173)\n",
      "Rescaling atlas mask from (1288, 841, 1090) to (1399, 903, 1173)\n",
      "Rescaling atlas mask from (1436, 814, 1136) to (1399, 903, 1173)\n",
      "Rescaling atlas mask from (1436, 859, 1113) to (1399, 903, 1173)\n",
      "Rescaling atlas mask from (1460, 723, 884) to (1399, 903, 1173)\n",
      "Rescaling atlas mask from (1288, 958, 1126) to (1399, 903, 1173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_29924\\640698736.py:9: RuntimeWarning: Mean of empty slice\n",
      "  group1_mean = np.nanmean(group1_hists, axis=0)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_29924\\640698736.py:10: RuntimeWarning: Mean of empty slice\n",
      "  group2_mean = np.nanmean(group2_hists, axis=0)\n",
      "c:\\Users\\admin\\anaconda3\\envs\\brainglobe-test\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# 3.1 Load and Mask Histograms for Both Groups\n",
    "##############################################################################\n",
    "\n",
    "group1_hists = load_group_voxels(group1_points_files, group1_atlas_files, atlas_shape)\n",
    "group2_hists = load_group_voxels(group2_points_files, group2_atlas_files, atlas_shape)\n",
    "\n",
    "# Calculate the mean and std only in brain regions (ignoring NaNs)\n",
    "group1_mean = np.nanmean(group1_hists, axis=0)\n",
    "group2_mean = np.nanmean(group2_hists, axis=0)\n",
    "\n",
    "group1_std = np.nanstd(group1_hists, axis=0)\n",
    "group2_std = np.nanstd(group2_hists, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_std = np.sqrt((group1_std**2 + group2_std**2) / 2)\n",
    "cohens_d = (group1_mean - group2_mean) / (pooled_std + 1e-9)  # Avoid /0\n",
    "\n",
    "# Replace NaNs with 0 (makes bright background?)\n",
    "cohens_d[np.isnan(cohens_d)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Cohen's d heatmap saved to masked_cohens_d_3d.tiff\n"
     ]
    }
   ],
   "source": [
    "output_file = \"masked_cohens_d_3d.tiff\"\n",
    "tifffile.imwrite(output_file, cohens_d.astype(np.float32))\n",
    "print(f\"Masked Cohen's d heatmap saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_points(points_file):\n",
    "    \"\"\"Load cell coordinates (Nx3) from an HDF5 atlas.points file.\"\"\"\n",
    "    df = pd.read_hdf(points_file, key=\"df\")\n",
    "    return df.values  # Nx3 array\n",
    "\n",
    "def subject_hist(points_file, atlas_shape):\n",
    "    \"\"\"\n",
    "    Convert a subject’s Nx3 points into a 3D histogram with shape=atlas_shape.\n",
    "    Bins = [0..atlas_shape[0]], [0..atlas_shape[1]], [0..atlas_shape[2]].\n",
    "    \"\"\"\n",
    "    coords = load_points(points_file)\n",
    "    x_bins = np.arange(0, atlas_shape[0] + 1)\n",
    "    y_bins = np.arange(0, atlas_shape[1] + 1)\n",
    "    z_bins = np.arange(0, atlas_shape[2] + 1)\n",
    "    hist, edges = np.histogramdd(coords, bins=[x_bins, y_bins, z_bins])\n",
    "    return hist\n",
    "\n",
    "def load_group_voxels(subject_list, atlas_shape):\n",
    "    \"\"\"\n",
    "    Return a list of 3D histograms (one per subject).\n",
    "    \"\"\"\n",
    "    all_hists = []\n",
    "    for subj in subject_list:\n",
    "        h = subject_hist(subj, atlas_shape)\n",
    "        all_hists.append(h)\n",
    "    return np.array(all_hists)  # shape: (num_subjects, X, Y, Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('Y:/public/projects/AnAl_20240405_Neuromod_PE/brainsaw/PE_mapping/PE/block2/026/brainreg_gr_4/points/atlas.points'), WindowsPath('Y:/public/projects/AnAl_20240405_Neuromod_PE/brainsaw/PE_mapping/PE/block2/833/brainreg_gr_3/points/atlas.points'), WindowsPath('Y:/public/projects/AnAl_20240405_Neuromod_PE/brainsaw/PE_mapping/PE/block2/987/brainreg_1/points/atlas.points'), WindowsPath('Y:/public/projects/AnAl_20240405_Neuromod_PE/brainsaw/PE_mapping/PE/block2/cFos61/brainreg_trained_trained_74model_changedparam_4/points/atlas.points'), WindowsPath('Y:/public/projects/AnAl_20240405_Neuromod_PE/brainsaw/PE_mapping/PE/block2/cFos74/brainreg_trained_BB_10/points/atlas.points')]\n"
     ]
    }
   ],
   "source": [
    "print(group1_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 52.1 GiB for an array with shape (5, 1320, 903, 1173) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m group1_hists \u001b[38;5;241m=\u001b[39m \u001b[43mload_group_voxels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup1_subjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matlas_shape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (N1, X, Y, Z)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m group2_hists \u001b[38;5;241m=\u001b[39m load_group_voxels(group2_subjects, atlas_shape)  \u001b[38;5;66;03m# shape (N2, X, Y, Z)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Mean voxel-wise cell counts\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[57], line 26\u001b[0m, in \u001b[0;36mload_group_voxels\u001b[1;34m(subject_list, atlas_shape)\u001b[0m\n\u001b[0;32m     24\u001b[0m     h \u001b[38;5;241m=\u001b[39m subject_hist(subj, atlas_shape)\n\u001b[0;32m     25\u001b[0m     all_hists\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_hists\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 52.1 GiB for an array with shape (5, 1320, 903, 1173) and data type float64"
     ]
    }
   ],
   "source": [
    "group1_hists = load_group_voxels(group1_subjects, atlas_shape)  # shape (N1, X, Y, Z)\n",
    "group2_hists = load_group_voxels(group2_subjects, atlas_shape)  # shape (N2, X, Y, Z)\n",
    "\n",
    "# Mean voxel-wise cell counts\n",
    "group1_mean = np.mean(group1_hists, axis=0)  # shape (X, Y, Z)\n",
    "group2_mean = np.mean(group2_hists, axis=0)\n",
    "\n",
    "# Std voxel-wise\n",
    "group1_std = np.std(group1_hists, axis=0)\n",
    "group2_std = np.std(group2_hists, axis=0)\n",
    "\n",
    "# (Optional) Convert to density\n",
    "#group1_mean /= group1_mean.sum()\n",
    "#group2_mean /= group2_mean.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_std = np.sqrt((group1_std**2 + group2_std**2) / 2)\n",
    "\n",
    "cohens_d = (group1_mean - group2_mean) / (pooled_std + 1e-9)  # avoid /0\n",
    "cohens_d[np.isnan(cohens_d)] = 0  # replace any NaNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "diff_meanden = (group1_mean - group2_mean)\n",
    "print(diff_meanden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved minimal-blur Cohen’s d volume to 'cohens_d_miniblur.tiff'.\n"
     ]
    }
   ],
   "source": [
    "import tifffile\n",
    "\n",
    "# Smaller sigma => less blur. sigma=1 => minimal smoothing, sigma=0 => none\n",
    "smoothed_cohens_d = gaussian_filter(cohens_d, sigma=10)\n",
    "\n",
    "# Save to 3D TIFF\n",
    "tifffile.imwrite(\"cohens_d_blur10.tiff\", smoothed_cohens_d.astype(np.float32))\n",
    "\n",
    "print(\"Saved minimal-blur Cohen’s d volume to 'cohens_d_miniblur.tiff'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "\n",
    "# Smaller sigma => less blur. sigma=1 => minimal smoothing, sigma=0 => none\n",
    "smoothed_diff_mean = gaussian_filter(diff_meanden, sigma=10)\n",
    "\n",
    "# Save to 3D TIFF\n",
    "tifffile.imwrite(\"diff_meanden_blur10.tiff\", smoothed_diff_mean.astype(np.float32))\n",
    "\n",
    "#print(\"Saved minimal-blur Cohen’s d volume to 'cohens_d_miniblur.tiff'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the raw data as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1399 but corresponding boolean dimension is 1320",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m region_mask \u001b[38;5;241m=\u001b[39m (atlas_mask \u001b[38;5;241m==\u001b[39m region_id)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the mean Cohen's d for each region\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m region_values \u001b[38;5;241m=\u001b[39m \u001b[43mcohens_d\u001b[49m\u001b[43m[\u001b[49m\u001b[43mregion_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m smoothed_values \u001b[38;5;241m=\u001b[39m smoothed_cohens_d[region_mask]\n\u001b[0;32m     17\u001b[0m mean_cohens_d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmean(region_values)\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 1399 but corresponding boolean dimension is 1320"
     ]
    }
   ],
   "source": [
    "\n",
    "atlas_mask_file = \"annotation_10.tiff\"\n",
    "atlas_mask = tifffile.imread(atlas_mask_file)\n",
    "\n",
    "# Extract unique region IDs from the annotation file (excluding 0 for background)\n",
    "unique_regions = np.unique(atlas_mask)\n",
    "unique_regions = unique_regions[unique_regions > 0]\n",
    "\n",
    "# Prepare data for CSV export\n",
    "region_data = []\n",
    "for region_id in unique_regions:\n",
    "    region_mask = (atlas_mask == region_id)\n",
    "    \n",
    "    # Calculate the mean Cohen's d for each region\n",
    "    region_values = cohens_d[region_mask]\n",
    "    smoothed_values = smoothed_cohens_d[region_mask]\n",
    "    \n",
    "    mean_cohens_d = np.nanmean(region_values)\n",
    "    mean_smoothed_cohens_d = np.nanmean(smoothed_values)\n",
    "    \n",
    "    region_data.append({\n",
    "        'Region ID': region_id,\n",
    "        \"Raw Cohen's d\": mean_cohens_d,\n",
    "        \"Smoothed Cohen's d\": mean_smoothed_cohens_d\n",
    "    })\n",
    "\n",
    "# Convert to a DataFrame\n",
    "region_df = pd.DataFrame(region_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "region_df.to_csv('cohens_d_by_region.csv', index=False)\n",
    "print(\"Saved raw and smoothed Cohen's d data to 'cohens_d_by_region.csv'.\")\n",
    "\n",
    "\n",
    "print(region_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_df.to_csv('cohens_d_by_region.csv', index=False)\n",
    "print(\"Saved raw data to 'cohens_d_by_region.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_29924\\2048137086.py:18: UserWarning: cohens_d_blur5_processed.tiff is a low contrast image\n",
      "  io.imsave(output_path, foreground_stack.astype(np.uint16))\n"
     ]
    }
   ],
   "source": [
    "import napari\n",
    "import numpy as np\n",
    "from skimage import io, filters\n",
    "\n",
    "# Load the image stack\n",
    "image_stack = io.imread(\"Y:/public/projects/AnAl_20240405_Neuromod_PE/brainsaw/PE_mapping/figures/cohens_d_blur5.tiff\")\n",
    "\n",
    "# Apply a threshold to remove the background\n",
    "threshold_value = filters.threshold_otsu(image_stack)\n",
    "foreground_mask = image_stack > threshold_value\n",
    "\n",
    "# Apply the mask to keep only the brain\n",
    "foreground_stack = image_stack * foreground_mask\n",
    "\n",
    "output_path = r\"cohens_d_blur5_processed.tiff\"\n",
    "\n",
    "# Save the image stack as a TIFF file\n",
    "io.imsave(output_path, foreground_stack.astype(np.uint16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_density_across_subjects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##############################################################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 4. Compute Mean Densities for Each Group\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m##############################################################################\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m group1_mean_hist, group1_all \u001b[38;5;241m=\u001b[39m \u001b[43mmean_density_across_subjects\u001b[49m(group1_subjects, atlas_shape)\n\u001b[0;32m      5\u001b[0m group2_mean_hist, group2_all \u001b[38;5;241m=\u001b[39m mean_density_across_subjects(group2_subjects, atlas_shape)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# If you want to normalize by total cell count per group, do:\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_density_across_subjects' is not defined"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# 4. Compute Mean Densities for Each Group\n",
    "##############################################################################\n",
    "group1_mean_hist, group1_all = mean_density_across_subjects(group1_subjects, atlas_shape)\n",
    "group2_mean_hist, group2_all = mean_density_across_subjects(group2_subjects, atlas_shape)\n",
    "\n",
    "# Normalize by total cell count per group\n",
    "group1_density = group1_mean_hist / group1_mean_hist.sum()\n",
    "group2_density = group2_mean_hist / group2_mean_hist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 5. Compute Cohen’s d for Each Voxel\n",
    "##############################################################################\n",
    "\n",
    "group1_std = np.std(group1_all, axis=0)  # shape: (X, Y, Z)\n",
    "group2_std = np.std(group2_all, axis=0)\n",
    "\n",
    "# Compute pooled std (unbiased version):\n",
    "pooled_std = np.sqrt((group1_std**2 + group2_std**2) / 2)\n",
    "\n",
    "# Cohen’s d: (mu1 - mu2) / pooled_std\n",
    "cohens_d = (group1_density - group2_density) / (pooled_std + 1e-9)  # to avoid /0\n",
    "# Replace NaNs\n",
    "cohens_d[np.isnan(cohens_d)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# 6. (Optional) Smooth the 3D Arrays\n",
    "##############################################################################\n",
    "smoothed_group1 = gaussian_filter(group1_density, sigma=2)\n",
    "smoothed_group2 = gaussian_filter(group2_density, sigma=2)\n",
    "smoothed_cohens_d = gaussian_filter(cohens_d, sigma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done computing mean densities and Cohen's d heatmap.\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# 7. Save or Visualize the Heatmaps\n",
    "##############################################################################\n",
    "# Option A: Using 'heatmap_from_points()' with “fake” points\n",
    "# 'heatmap_from_points()' expects Nx3 points array. \n",
    "#  convert 3D array to points, weighting each voxel by the density or Cohen’s d value (is this correct?).\n",
    "\n",
    "def array_to_points_3d(volume_3d):\n",
    "    \"\"\"\n",
    "    Convert a 3D volume into a list of points, \n",
    "    repeating each voxel coordinate according to the voxel value (if integer).\n",
    "    \n",
    "    But for continuous data (like densities, we might store them as 'weights').\n",
    "    For 'heatmap_from_points()', we can pass 'points' & 'weights' with histogramdd too.\n",
    "    \"\"\"\n",
    "    points = []\n",
    "    for x in range(volume_3d.shape[0]):\n",
    "        for y in range(volume_3d.shape[1]):\n",
    "            for z in range(volume_3d.shape[2]):\n",
    "                val = volume_3d[x, y, z]\n",
    "                # If val is small or zero, skip\n",
    "                if val > 0:\n",
    "                    points.append([x, y, z])\n",
    "    return np.array(points, dtype=float)\n",
    "\n",
    "# Convert the smoothed Cohen’s d array to \"points\"\n",
    "cohens_d_points = array_to_points_3d(smoothed_cohens_d)\n",
    "\n",
    "# Then call 'heatmap_from_points()'\n",
    "heatmap_from_points(\n",
    "    points=cohens_d_points,\n",
    "    image_resolution=1.0,  # 1 voxel = 1 unit (one cell?)\n",
    "    image_shape=atlas_shape,\n",
    "    output_filename=\"cohens_d_heatmap.tiff\",\n",
    "    smoothing=0  #  already smoothed in 3D\n",
    ")\n",
    "\n",
    "# Option B: Save 'smoothed_cohens_d' to .npy:\n",
    "# np.save(\"cohens_d_3d.npy\", smoothed_cohens_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 total cells: 6964342\n",
      "[[1330  593  445]\n",
      " [1348  425  576]\n",
      " [1334  451  585]\n",
      " ...\n",
      " [  76  600  349]\n",
      " [ 139  580  720]\n",
      " [1231  478  660]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from brainglobe_utils.image.heatmap import heatmap_from_points\n",
    "\n",
    "group1_subjects = [\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\026\\brainreg_gr_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\833\\brainreg_gr_3\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\987\\brainreg_1\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos61\\brainreg_trained_trained_74model_changedparam_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\PE\\block2\\cFos74\\brainreg_trained_BB_10\\points\\atlas.points\"),\n",
    "]\n",
    "\n",
    "# Combine all subject points\n",
    "all_group1_points = []\n",
    "for subj in group1_subjects:\n",
    "    # Load downsampled (x,y,z)\n",
    "    df = pd.read_hdf(subj, key=\"df\")\n",
    "    points_array = df.values  # shape: (N, 3)\n",
    "    all_group1_points.append(points_array)\n",
    "\n",
    "# Stack\n",
    "all_group1_points = np.vstack(all_group1_points)\n",
    "print(\"Group 1 total cells:\", all_group1_points.shape[0])\n",
    "print( all_group1_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 2 total cells: 2005181\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from brainglobe_utils.image.heatmap import heatmap_from_points\n",
    "\n",
    "group2_subjects = [\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\834\\brainreg_gr_mixed_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\838\\brainreg_gr_4\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\981\\brainreg_3\\points\\atlas.points\"),\n",
    "    Path(r\"Y:\\public\\projects\\AnAl_20240405_Neuromod_PE\\brainsaw\\PE_mapping\\Control\\cFos60\\brainreg_trained_after74_6\\points\\atlas.points\")\n",
    "]\n",
    "\n",
    "# Combine all subject points\n",
    "all_group2_points = []\n",
    "for subj in group2_subjects:\n",
    "    # Load downsampled (x,y,z)\n",
    "    df = pd.read_hdf(subj, key=\"df\")\n",
    "    points_array = df.values  # shape: (N, 3)\n",
    "    all_group2_points.append(points_array)\n",
    "\n",
    "# Stack\n",
    "all_group2_points = np.vstack(all_group2_points)\n",
    "print(\"Group 2 total cells:\", all_group2_points.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved group1_mean_density.tiff for Group 1.\n"
     ]
    }
   ],
   "source": [
    "image_shape = (1399, 903, 1173)  # Adjust to match  downsampled atlas (get the standard space values 1320 ... ....)\n",
    "voxel_resolution = 10          # Allen Mouse Brain 10 µm\n",
    "\n",
    "heatmap_from_points(\n",
    "    points=all_group1_points,\n",
    "    image_resolution=voxel_resolution,\n",
    "    image_shape=image_shape,\n",
    "    output_filename=\"group1_mean_density.tiff\",\n",
    "    smoothing=100,        # sigma=100/25=4 voxels\n",
    "    bin_sizes=(1,1,1),    \n",
    "    mask_image=None,      # or load a mask\n",
    ")\n",
    "print(\"Saved group1_mean_density.tiff for Group 1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved group2_mean_density.tiff for Group 2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "heatmap_from_points(\n",
    "    points=all_group2_points,\n",
    "    image_resolution=voxel_resolution,\n",
    "    image_shape=image_shape,\n",
    "    output_filename=\"group2_mean_density.tiff\",\n",
    "    smoothing=100,\n",
    "    bin_sizes=(1,1,1),   \n",
    "    mask_image=None,  \n",
    ")\n",
    "print(\"Saved group2_mean_density.tiff for Group 2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Cohen's d heatmap saved to masked_cohens_d_3d.tiff\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tifffile\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Load the registered atlas file from registartion folder\n",
    "\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "# Load the Cohen's d heatmap\n",
    "cohens_d_file = \"cohens_d_blur5.tiff\"\n",
    "cohens_d_data = tifffile.imread(cohens_d_file)\n",
    "\n",
    "# Load the atlas mask \n",
    "atlas_mask_file = \"Y:/public/projects/AnAl_20240405_Neuromod_PE/brainsaw/PE_mapping/PE/block2/833/brainreg_gr_3/registration/registered_atlas.tiff\"\n",
    "\n",
    "atlas_mask = tifffile.imread(atlas_mask_file)\n",
    "\n",
    "# Ensure the mask is binary\n",
    "atlas_mask = (atlas_mask > 0).astype(np.uint8)\n",
    "\n",
    "# Check if shapes match\n",
    "# rescale the atlas mask\n",
    "if atlas_mask.shape != cohens_d_data.shape:\n",
    "    print(f\"Rescaling atlas mask from {atlas_mask.shape} to {cohens_d_data.shape}\")\n",
    "    scaling_factors = np.array(cohens_d_data.shape) / np.array(atlas_mask.shape)\n",
    "    atlas_mask = zoom(atlas_mask, scaling_factors, order=0)\n",
    "\n",
    "# Apply the mask to the Cohen's d data\n",
    "masked_cohens_d = cohens_d_data * atlas_mask\n",
    "\n",
    "# Background (non-brain) regions to NaN (makes background bright?)\n",
    "masked_cohens_d[atlas_mask == 0] = np.nan\n",
    "\n",
    "# Save the masked data as TIFF\n",
    "output_file = \"masked_cohens_d_3d.tiff\"\n",
    "tifffile.imwrite(output_file, masked_cohens_d.astype(np.float32))\n",
    "\n",
    "print(f\"Masked Cohen's d heatmap saved to {output_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainglobe-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
